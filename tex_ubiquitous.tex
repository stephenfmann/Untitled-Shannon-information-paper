\section{Agnostic information}\label{sec:agnostic}

\subsection{How scientists use information theory}\label{subsec:scientists}

Around the same time that Bar-Hillel \& Carnap were discussing the relevance of Shannon's work for philosophy, scientists noticed that his mathematical tools were of use beyond the context of communications engineering.
For example, entropy, originally devised as a measure of how many code symbols would be needed to represent a sequence of source outcomes, was given a rather more general interpretation as a measure of the uncertainty associated with a probability space.
Intuitively, when entropy is high, an observer is less sure which outcome will occur, so entropy seems a reasonable measure of uncertainty.
Further uses can be derived from this more general interpretation.
Entropy has been used in ecology as a measure of diversity: when we treat the proportions of different species in an ecological population as probabilities, the entropy of that population measures uncertainty about which species would be observed if one sampled randomly from the population.
This intuitively captures an aspect of the population diversity \citep{margalef1957information}.

Shifts in the interpretation of entropy and other measures accompanied the emergence of the term \textbf{information theory} to describe Shannon's mathematical tools and their more general application across the sciences (see figure \ref{fig:info_theory}).
It is nowadays customary for philosophers to use the term `information theory' to describe both these more general applications and the original context of communications engineering in which they were devised.
I will continue to distinguish communication theory from information theory, because the domain of application of the measures affects the truth values of philosophical claims made about them. % This is a promissory note which should be paid off!

\input{fig_info_theory}

In the context of information theory, informational measures are often treated as statistical tools.
Perhaps the most well-known is \textbf{mutual information}, typically interpreted as a measure of the strength of correlation between two variables.
Mutual information has been employed in a diverse range of sciences, including:

\begin{itemize}
    \item Behavioural ecology, to measure the correlation between the honeybee waggle dance and the location of food sources \citep{haldane1954statistical}
    \item Molecular biology, to measure the correlation between inputs and outputs of a quorum-sensing bacterium \citep{mehta2009information}
    \item Evolutionary biology, to show that the correlation between an environmental cue and a fitness-relevant state of affairs is an upper bound on the increased growth rate of an organism conditioning its behaviour on the cue \citep{donaldson-matasci2010fitness}
    \item Cosmology, to measure the correlation between galaxies' internal morphology and their local environments \citep{pandey2017how}
    \item Linguistics, to measure the co-occurrence of words in a corpus \citep[$\S$4]{hunston2002corpora}
    \item Neuroscience, to measure the correlation between neural firings and environmental states \citep[][and references therein]{rathkopf2017neural}
\end{itemize}

\noindent That many things correlate in the natural world is a useful fact for science to exploit.
Correlations increase the opportunity to discover features about a target phenomenon.
If the target is difficult to observe, it might more easily be studied by observing something that correlates with it. 
Furthermore, correlations play a special role in the functional sciences.
Biological and cognitive agents can condition their behaviour on cues and signals that correlate with success-relevant states of affairs, and scientists are therefore interested in how correlations are exploited in general.
Mutual information and related measures are some of the most common tools employed in these investigations.

The breadth of application of mutual information is at the heart of the second source of scepticism about the relevance of communication theory for naturalist representation.
Scholars usually move from a premise about mutual information to a conclusion about information theory as a whole -- which is then seen as encompassing communication theory.
Because philosophers are rarely explicit about the dialectical moves required to reach this conclusion, in the next few subsections I have tried to tease apart the intended argument in order to refute it.
As a result, my discussion concerns one possible way of moving from claims about mutual information to scepticism about the relevance of communication theory.
There may be other, better arguments that my account here does not affect.

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\subsection{Mutual information cannot distinguish signals and cues}

The wide applicability of mutual information generates scepticism about the relevance of communication theory for accounts of representation.
To demonstrate how, it will help to introduce one more distinction, between \textbf{signals} and \textbf{cues}.

Originating in behavioural ecology, the signal/cue distinction highlights the fact that some informational vehicles have the function to provide the information they do, whereas some are `accidentally' informational, used opportunistically by their receivers \citep[$\S$1.2]{maynardsmith2003animal}.
Vehicles selected to serve a communicative role are called signals, while vehicles that fortuitously provide information are called cues.
(In philosophy the term `natural sign' is sometimes used; I am here using `cue' to cover all cases described by that term.)
For example, the waggle dance is a signal because it evolved in the honeybee lineage to serve as an informational vehicle that enables workers to enjoy greater success at foraging or nest-finding \citep{gould1975honey,riley2005flight}.
In contrast, bees' use of the position of the sun in the sky to navigate is a cue, because the sun's location is not an outcome of a process of selection that jointly produced both it and the bees' navigational behaviour.

Scepticism about the relevance of communication theory for naturalist representation gains traction when we note that mutual information quantifies the strength of a correlation no matter whether its vehicles are signals or cues.
That a vehicle has positive mutual information with an environmental state does not determine whether that vehicle is a cue or a signal.
The aforementioned uses of mutual information across the sciences emphasise this point.
While the correlation between the waggle dance and food locations is due to the fact that the waggle dance is a signal, the correlation between galaxies' morphology and their local environment is clearly not.
The vehicle in the evolutionary model of \citet{donaldson-matasci2010fitness} is definitionally a cue.
The co-occurrence of words in a corpus is not itself a signal, though the words themselves are representations (or at least combine to produce representations).
Without further detail, it is not clear whether the output of a quorum-sensing bacterium counts as a signal of its input; nonetheless, the mutual information between the two can be measured.
Neural firings are sometimes claimed to be representations, but simply measuring the correlation between them and environmental states is not sufficient to establish this \citep{rathkopf2017neural}.

So one cannot move from a claim about mutual information to a claim about signalhood.
From the mere fact that two things bear a correlational relationship, no conclusion can be drawn about whether one is a signal of the other.
For the avoidance of doubt, I agree with this point, and suggest encapsulating it as follows:

\begin{myquote}
{\sc Agnostic Mutual Information}: Mutual information cannot distinguish signals and cues.
\end{myquote}

\noindent Treating {\sc Agnostic Mutual Information} as uncontroversially true, the remainder of this section concerns two related claims:

\begin{myquote}
{\sc Agnostic Information Theory}: Information theory cannot distinguish signals and cues.
\end{myquote}

\begin{myquote}
{\sc Agnostic Communication Theory}: Communication theory cannot distinguish signals and cues.
\end{myquote}

\noindent With these claims on the table, the remainder of the section has the following goals:

\begin{enumerate}
    \item Demonstrating that many scholars move from {\sc Agnostic Mutual Information} to {\sc Agnostic Information Theory} by employing the vague term `Shannon information'.
    \item Demonstrating that scholars diverge widely on the conclusions they draw from {\sc Agnostic Information Theory}.
    \item Demonstrating that, regardless of the propriety of the above move, a further move from {\sc Agnostic Information Theory} to {\sc Agnostic Communication Theory} should be roundly rejected because {\sc Agnostic Communication Theory} is false.
\end{enumerate}

\noindent The following subsections address these points in turn.

%%%%%%
%%%%%%
%%%%%%
%%%%%%
\subsection{\sloppy Philosophers move from {\sc Agnostic Mutual Information} to {\sc Agnostic Information Theory} by employing the vague term `Shannon information'}

Several authors appear to move from {\sc Agnostic Mutual Information}, a claim about a specific measure, to {\sc Agnostic Information Theory}, a claim about a collection of mathematical tools and concepts.
How do they achieve this?
I believe the move is made by employing the term `Shannon information' to refer to both mutual information specifically and all the concepts and tools of information theory in general.
Because `Shannon information' is given diverse definitions, demonstrably true claims about mutual information can be interpreted as claims about information theory as a whole.

Table \ref{tab:shannon} lists some of the definitions of `Shannon information' in contemporary philosophy.

%% I'm putting the table here because it was causing a huge gap in the text.
\input{tab_shannon}

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
\subsection{\sloppy Philosophers diverge on the conclusions drawn from {\sc Agnostic Information Theory}}

Contemporary scholars draw different conclusions from {\sc Agnostic Information Theory}.
\citet{skyrms2010signals} and \citet{isaac2018semantics} flat-footedly assert that mutual information just \textit{is} semantic content, and that it turns out content inheres in vehicles other than signals after all.
\citet[SECTION]{shea2018representation} outlines a set of extra conditions that a vehicle must meet in order to be considered a representation. 
\citet{lean2014shannon} and \citet[CHAPTER]{hutto2017evolving} accept that correlations between signalling vehicles and signifieds can do explanatory work in the functional sciences, but deny that functional correlations amount to semantic content.
Although the conclusions drawn vary widely, all these scholars share a premise: that the only tools available to information theory are mutual information and related measures.
In effect, the sceptical claim is that all of the tools of information theory are insufficient to distinguish signals and cues.

Although I believe the move from {\sc Agnostic Mutual Information} to {\sc Agnostic Information Theory} is illicit without further premises, and that the term `Shannon information' should be used more consistently, I am going to set these points aside.
I'm not concerned with the truth value of the intermediate claim: what I want to show is that despite {\sc Agnostic Mutual Information} being true, {\sc Agnostic Communication Theory} is false.

In sum, scepticism about the relevance of communication theory for naturalist approaches to content results from two illicit moves: first, from a claim about mutual information to a claim about the whole of information theory via the term `Shannon information'; second, from the resultant claim about information theory to a claim about communication theory via treating the two disciplines as identical.
The next section shows that the resultant claim about communication theory is simply false.
Communication theory can and does distinguish signals and cues.


%%%%%%
%%%%%%
%%%%%%
%%%%%%
\subsection{{\sc Agnostic Communication Theory} is false}

Whereas information theory is a collection of mathematical tools with wide application across the sciences, communication theory is an engineering discipline with the specific goal of designing efficient signalling techniques.
The vehicles transmitted in the central model are definitionally signals, not cues.
The sender produces the vehicle in accordance with an encoding scheme shared with the receiver.
This definition of `signal' is shared by several sciences \citep{shannon1948mathematicala,maynardsmith2003animal} and philosophical accounts \citep[$\S$6]{millikan2004varieties} \citep{bergstrom2011transmission}.

Not only does the formal apparatus of communication theory distinguish signals and cues, its theorems distinguish them too.
Consider, on the one hand, the \textbf{second theorem} and \textbf{third theorem} of communication theory, which require that the vehicles they address be signals, and on the other hand \textbf{Kelly's theorem}, which requires only that the vehicle be a cue.

The second theorem (sometimes called the noisy channel theorem) determines what level of performance the receiver is able to achieve thanks to the sender's encoding scheme, in the face of a channel that inserts noise into the vehicle.
The theorem answers a question about receiver performance by attending to the sender's design of the vehicle: different encoding schemes would yield different performance levels.
The vehicle mentioned in this theorem is definitionally and necessarily a signal.

The third theorem (also known as the rate-distortion theorem) addresses a similar question, this time with the added benefit that the receiver need not achieve perfect performance.
Suppose for example that the receiver only needs to reconstruct four out of every five outcomes produced by the source.
The third theorem states that it is possible to determine the minimum transmission rate that the sender must ensure in order for the receiver to perform at the specified level.
Again, the theorem assumes that the transmission rate is tunable by the sender's choice of encoding scheme.
By invoking a vehicle whose form can be adapted to performance specifications, both the second and third theorems employ a concept of signal rather than cue.

Kelly's theorem, by contrast, concerns the performance of a receiver conditioning its behaviour on a vehicle that bears a fixed level of mutual information with a signified \citep{kelly1956new}.
The theorem states that the mutual information is an upper bound on the performance improvement a receiver can enjoy by using the cue rather than not using it.
Because the emphasis is on receiver behaviour, and the mutual information is held fixed, the vehicle is conceptualised as a cue.
The theorem has been applied in evolutionary biology \citep{donaldson-matasci2010fitness} where again the vehicle in question is treated as an environmental cue rather than a signal.

Of course, nothing prevents us applying Kelly's theorem to a signal too.
The claim is not that theorems about cues do not apply to signals, but that theorems about signals do not apply to cues.
There is an asymmetry in the definition of signals and cues, and the two kinds of theorem are asymmetric in a way that reflects that.
Any vehicle can be treated as a cue simply by failing to specify whether or not it was designed for communicative use.
That is what Kelly's theorem does, in the guise of keeping the mutual information fixed and asking how the receiver can make use of it.
In contrast, the second and third theorems require that vehicles be signals because they assure a level of functional performance that is only available when the sender is able to tune the signal's production to both the features of the channel and the receiver's needs.

In sum, communication theory distinguishes signals from cues both by providing the means to define signals and by employing theorems that require the vehicles in question be signals.

%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
\subsection{Signal content is not determined by mutual information}

What can the naturalist philosopher conclude from the fact that communication theory distinguishes signals and cues?

Before they conclude anything, they should note that nothing we have said so far implies that signal content is determined by mutual information.
Some contemporary treatments view content as being constructed from mutual information, typically by adding constraints such as that the vehicle has the function to carry it.
However, as we saw earlier, the pretheoretic notion of representation used in communication theory treats the content of a signal as the source string it encodes.
This result is given a theoretical underpinning by teleosemantics, and neither the pretheoretic definition nor the teleosemantic account treats mutual information as part of the definition of signal content.
It is the encoding scheme, not the correlation, that determines the content of a signal.

In this light, it is true but irrelevant that both signals and cues bear mutual information with their signifieds. 
The content of a signal is not determined by reference to mutual information at all!
It is determined by the encoding scheme by which it was designed.
While it is true that the signalling systems we usually discuss do indeed involve signals that bear mutual information with source events, it is not in virtue of this that an individual signal has the content it does.
A signal's content is instead determined by the encoding scheme in accordance with which the sender produced it.


\begin{itemize}
    \item These points are reflected in the mathematics: when you do $\max_{p(x)}I(X;Y)$ in the second theorem you are treating $X$ and $Y$ as either end of a signalling channel; when you keep $I(X;Y)$ fixed in Kelly's theorem you are treating $Y$ as a cue of $X$.
    \item There is not yet a mathematical measure that determines whether a thing is a signal or a cue, but the mathematics does distinguish whether something is being modelled as a signal or a cue, and that's good enough for philosophers to care about it.
    \item Mutual information is a measure between types, while semantic content is typically construed as a relation between tokens.
\end{itemize}

In sum, true premises about the broad application of mutual information in science entail nothing about the relevance of communication theory for naturalist approaches to content.

