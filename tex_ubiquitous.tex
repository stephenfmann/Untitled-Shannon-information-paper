\section{Agnostic information}\label{sec:agnostic}

The second route to scepticism about the relevance of communication theory for naturalist representation begins with the claim that mathematical measures defined within communication theory cannot distinguish between representations and non-representations.
In other words, information is agnostic to representational status.
In this section I argue that although this is true of certain mathematical functions like mutual information, communication theory has many more tools at its disposal.
Communication theory does distinguish between signals and non-signals -- it must do in order for its theorems to have sense.
The theory is fundamentally about the costs and benefits of representation and how to trade them off judiciously.
Philosophers undersell the resources available to communication theory by focusing solely on a small set of measures.

\subsection{How scientists use information theory}\label{subsec:scientists}

Around the same time Bar-Hillel \& Carnap were discussing the relevance of Shannon's work for philosophy, scientists began to notice that his mathematical tools were of use beyond the context of communications engineering.
For example, entropy, originally devised as a measure of how many code symbols would be needed to represent a sequence of source outcomes, was given a rather more general interpretation as a measure of the uncertainty associated with a probability space.
Intuitively, when entropy is high, an observer is less sure which outcome will occur, so entropy seems a reasonable measure of uncertainty.
Further uses can be derived from this more general interpretation.
Entropy has been used in ecology as a measure of diversity: when we treat the proportions of different species in an ecological population as probabilities, the entropy of that population measures our uncertainty about which species would be observed if we sampled randomly from the population.
This intuitively captures an aspect of the population diversity \citep{margalef1957information}.

Shifts in the interpretation of entropy and other measures accompanied the emergence of the term \textbf{information theory} to describe Shannon's mathematical tools and their more general application across the sciences.
Today, information theory comprises a set of concepts and measures common to many mathematical and scientific disciplines (see figure \ref{fig:info_theory}).
For better or worse, it has become customary for philosophers to use the term `information theory' to describe both these more general applications and the original context of communications engineering in which they were devised.
Since the claims I make below depend on this distinction being upheld, I will continue to use `information theory' and `communication theory' non-synonymously.

\input{fig_info_theory}

In the context of information theory, informational measures are often treated as statistical tools.
Perhaps the most well-known is \textbf{mutual information}, typically interpreted as a measure of the strength of correlation between two variables.
Mutual information has been employed in a diverse range of sciences, including:

\begin{itemize}
    \item Behavioural ecology, to measure the correlation between the honeybee waggle dance and the location of food sources \citep{haldane1954statistical}
    \item Evolutionary biology, to show that the correlation between an environmental cue and a fitness-relevant state of affairs is an upper bound on the increased growth rate of an organism conditioning its behaviour on the cue \citep{donaldson-matasci2010fitness}
    \item Cosmology, to measure the correlation between galaxies' internal morphology and their local environments \citep{pandey2017how}
    \item Linguistics, to measure the co-occurrence of words in a corpus \citep[$\S$4]{hunston2002corpora}
    \item Molecular biology, to measure the correlation between inputs and outputs of a quorum-sensing bacterium \citep{mehta2009information}
    \item Neuroscience, to measure the correlation between neural firings and environmental states \citep[][and references therein]{rathkopf2017neural}
\end{itemize}

% \noindent That many things correlate in the natural world is a useful fact for science to exploit.
% Correlations increase the opportunity to discover features about a target phenomenon.
% If the target is difficult to observe, it might more easily be studied by observing something that correlates with it. 
% Furthermore, correlations play a special role in the functional sciences.
% Biological and cognitive agents can condition their behaviour on cues and signals that correlate with success-relevant states of affairs, and scientists are therefore interested in how correlations are exploited in general.
% Mutual information and related measures are some of the most common tools employed in these investigations.

\noindent The breadth of application of mutual information is at the heart of a second source of scepticism about the relevance of communication theory for naturalist representation.
Scholars usually move from a premise about mutual information to a conclusion about information theory as a whole -- which is then seen as encompassing communication theory.
Because philosophers are rarely explicit about the dialectical moves required to reach this conclusion, in the next few subsections I have tried to tease apart the intended argument in order to refute it.
As a result, my discussion focuses on one possible way of moving from claims about mutual information to scepticism about the relevance of communication theory.
There may be other, better arguments that my account does not affect.

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\subsection{Mutual information cannot distinguish signals and cues}

Let us begin with a premise that sceptics and optimists alike should agree on: mutual information cannot distinguish between \textbf{signals} and \textbf{cues}.

Originating in behavioural ecology, the signal/cue distinction highlights the fact that some informational vehicles have the function to provide the information they do, whereas some are `accidentally' informational, used opportunistically by their receivers \citep[$\S$1.2]{maynardsmith2003animal}.
Vehicles selected to serve a communicative role are called signals, while vehicles that fortuitously provide information are called cues.
(In philosophy the term `natural sign' is sometimes used; I am here using `cue' to cover all cases described by that term.)
The waggle dance is a signal because it evolved in the honeybee lineage to serve as an informational vehicle that enables workers to enjoy greater success at foraging or nest-finding \citep{gould1975honey,riley2005flight}.
In contrast, bees' use of the position of the sun in the sky to navigate is a cue, because the sun's location is not an outcome of a process of selection that jointly produced both it and the bees' navigational behaviour.

Mutual information quantifies the strength of a correlation no matter whether its vehicles are signals or cues.
That a vehicle has positive mutual information with an environmental state does not determine whether that vehicle is a signal or a cue.
The sheer variety of scientific contexts in which mutual information is used emphasises this point.
While the correlation between the waggle dance and food locations is due to the fact that the waggle dance is a signal, the correlation between galaxies' morphology and their local environment clearly is not.
The vehicle in the evolutionary model of \citet{donaldson-matasci2010fitness} is definitionally a cue.
The co-occurrence of words in a corpus is not itself a signal, though the words themselves are representations (or at least combine to produce representations).
Without further detail, it is not clear whether the output of a quorum-sensing bacterium counts as a signal of its input; nonetheless, mutual information between the two can be measured.
Neural firings are sometimes claimed to be representations, but simply measuring the correlation between them and environmental states is not sufficient to establish this \citep{rathkopf2017neural}.

From the mere fact that two things bear a correlational relationship, no conclusion can be drawn about whether one is a signal of the other.
% In other words, one cannot move from a claim about mutual information to a claim about signalhood.
For the avoidance of doubt, I agree with this point, and suggest encapsulating it as follows:

\begin{myquote}
\ami: Mutual information cannot distinguish signals and cues.
\end{myquote}

\noindent The path to scepticism I want to explore is the move from \ami{} to one or both of the following claims:

\begin{myquote}
\ait: Information theory cannot distinguish signals and cues.
\end{myquote}

\begin{myquote}
\act: Communication theory cannot distinguish signals and cues.
\end{myquote}

\noindent The latter claim would certainly challenge the relevance of communication theory for theories of representation.
If communication theory cannot distinguish signals and cues, then there is little hope of it distinguishing any of the more sophisticated kinds of representation of interest to cognitive scientists and philosophers; if it cannot distinguish them, likely it cannot say anything philosophically interesting about them.

I am going to argue firstly that \ami{} does not entail \act{}, and secondly that \act{} is false.
Of course, the second conclusion would immediately entail the first (because we are treating \ami{} as uncontroversially true), but in laying out the first argument we can explore the intermediate claim \ait{}.
This allows us to describe the contemporary philosophical landscape in relation to the account I am endorsing.
Most philosophical approaches share a premise that I reject (and that I assume Mart\'{i}nez rejects): that the only way communication theory could be used to naturalise representation is by constructing semantic content from correlational relationships.

The two arguments commence in the following two subsections.

%%%%%%
%%%%%%
%%%%%%
%%%%%%
\subsection{\sloppy \ami{} does not entail \act{}}

Let me lay out my suspicions clearly: I suspect that philosophers move from \ami{} to the intermediate claim \ait{} by employing the term `Shannon information' to mean both `mutual information' and `all of the tools and concepts available to information theory'.
I further suspect that philosophers then move from \ait{} to \act{} by treating `information theory' and `communication theory' as synonymous.

`Shannon information' is given diverse definitions in philosophy (table \ref{tab:shannon}).
By invoking it, demonstrably true claims about mutual information can be interpreted as unsupported claims about information theory as a whole.
\citet[$\S$2]{godfrey-smith2016biological} define ``Shannon's concept of information'' (also using the hyphenated term ``Shannon-information'') in terms of correlational relationships like mutual information; they characterise it as ``the sense of information isolated by Claude Shannon and used in mathematical information theory'' \citep[1]{godfrey-smith2016biological}.
% They say: ``This sense of information is associated with Claude \citet{shannon1948mathematicalc} who showed how the concept of information could be used to quantify facts about contingency and correlation in a useful way, initially for use in communication technology.''
\citet[759]{owren2010redefining} describe ``Shannon and Weaver's \parencite*{shannon1949mathematical} theory of information'' and say ``the associated concept of \textit{Shannon information} refers strictly and solely to observable correlations between events in the world.''
\citet[$\S$6.1]{dennett2017bacteria} says that ``Shannon's theory is, at its most fundamental, about the statistical relationship between different states of affairs in the world: What can be gleaned (in principle) about state A from the observation of state B?'', later explicitly distinguishing Shannon information from semantic information.
\citet[p. 12, n. 11]{shea2018representation} says that ``\citet{shannon1948mathematicalc} developed a formal treatment of correlational information -- as a theory of communication, rather than meaning -- which forms the foundation of (mathematical) information theory'', later invoking ``Shannon information'' to describe a correlational measure consistent with the definition of mutual information \citep[p. 78, n. 5]{shea2018representation}.
These examples are admittedly cherry-picked; my claim is that the assumption underlying the above quotes is widely shared.
Scholars may not assert \ait{} explicitly, but by running together mutual information with information theory as a whole they make the transition much easier to swallow.

\input{tab_shannon}

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
% \subsection{\sloppy Philosophers diverge on the conclusions drawn from {\sc Agnostic Information Theory}}

Although \ait{} is generally agreed upon, different authors draw very different conclusions from it.
I want to mention three popular lines of thought in order to distinguish them from the account that this paper is ultimately designed to support:

\begin{itemize}
    \item Information theory cannot distinguish signals and cues, but since information theory is our best chance of naturalising semantic content, informational measures alone must define semantic content; therefore, even cues possess semantic content \citep{skyrms2010signals,isaac2018semantics}
    \item Information theory cannot distinguish signals and cues, but since information theory is our best chance of naturalising semantic content, extra conditions must be added to informational measures before they constitute semantic content \citep[$\S$3-4]{shea2018representation}; \citep[pp. 6-9,34-36]{neander2017mark}
    \item Information theory cannot distinguish signals and cues, therefore informational measures are not the right way to naturalise semantic content \citep{lean2014shannon}; \citep[$\S$4]{hutto2013radicalizing} 
\end{itemize}

\noindent These three views share a premise: that the only tools available to information theory are mutual information and related measures.
I reject the assumption that the only resources offered by Shannon's formal work to naturalist theories of representation are measures of correlational information.\footnote{\citet[$\S$5]{shea2018representation} discusses structural correspondence as a non-correlational source of semantic content, and suggests other possible sources \citep[p. 76 n. 1]{shea2018representation}. But he appears to share the premise that, among the tools available to information theory, measures of correlation are all that is relevant for naturalist representation.}
The view I proposed in the section on Shannon's Warning can therefore be understood as:

\begin{itemize}
    \item Regardless whether information theory can distinguish signals and cues, the semantic content of a signal in a communication-theoretic model is not defined in terms of informational measures.
\end{itemize}

\noindent 
In effect, the sceptical claim is that all of the tools of information theory are insufficient to distinguish signals and cues.
This may be true depending on how the limits of information theory are drawn, but it leads to a false conclusion when information theory is conflated with communication theory.
In failing to distinguish the theories, the works cited in table \ref{tab:shannon} shorten the path to this further inference.
An exception is \citet[17-20]{piccinini2011information} which explicitly distinguishes the theories.
In the other works cited in table \ref{tab:shannon}, one can find the following terms describing an indiscriminate hybrid of information theory and communication theory:

\begin{itemize}
    \item Information theory \citep[p. 3 passim]{adriaans2019information}; \citep[12]{shea2018representation}; \citep[614]{timpson2006grammar}; \citep[2]{baker2021natural}; \citep[3]{kirchhoff2021universal}; \citep[$\S$6]{dennett2017bacteria}; \citep[1]{isaac2018semantics}; \citep[8]{godfrey-smith2016biological}; \citep[p. 330, n. 8 passim]{rathkopf2017neural} \citep[p. 777 as ``this formal information theory'']{owren2010redefining}; \citep[p. 1991 as ``the theory of information'']{lombardi2015shannon}
    \item Mathematical information theory \citep[1]{godfrey-smith2016biological}
    \item Communication theory \citep[592]{timpson2006grammar}; \citep[1987]{lombardi2015shannon}
    \item the mathematical theory of communication \citep[1]{floridi2019semantic}; \citep[322]{rathkopf2017neural}; \citep[1988]{lombardi2015shannon}
    \item Shannon's theory \citep[2]{isaac2018semantics}; \citep[1984]{lombardi2015shannon}; Shannon information theory \citep[400]{lean2014shannon}; Shannon's theory of information \citep[p. 78, n. 5]{shea2018representation} and \citep[6]{isaac2018semantics}; Shannon's mathematical theory of information \citep[$\S$1.1]{dennett2017bacteria}; ``the Shannon theory'' \citep[p. 599 n. 15]{timpson2006grammar}
    \item ``the Shannon–Weaver theory of communication'' \citep[p. 756 n. 3]{owren2010redefining}
    \item ``Shannon and Weaver's \parencite*{shannon1949mathematical} theory of information'' \citep[759]{owren2010redefining}; ``Shannon and Weaver's quantitative-information theory'' \citep[761]{owren2010redefining}; an earlier work of Dennett not cited in the table describes ``Shannon-Weaver information theory'' \citep[344]{dennett1983intentional}
    \item ``Shannon–Weiner theory'' \citep[19]{baker2021natural}; either a typo (\textit{Weiner} instead of \textit{Wiener}) or a conflation of Norbert Wiener and Warren Weaver
\end{itemize}

\noindent In my opinion these varied terms reveal a tendency to conflate information theory with communication theory.
The tendency is not universal, as \citet[17-20]{piccinini2011information} explicitly distinguish the theories.
Again this examples are cherry-picked and only indicative.
I am treating them as evidence for the claim that philosophers slip too easily between \ait{} and \act{} without sufficient argument.

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%% Expand on the above views something like as follows ONLY IF it seems necessary/interesting/useful for the paper as a whole.
%% Finish the paper without the following, then consider whether it is improved by adding it back in.

% I will briefly describe each in order to distinguish the view I am defending.

% \citet{skyrms2010signals} and \citet{isaac2018semantics} flat-footedly assert that mutual information just \textit{is} semantic content, and that it turns out content inheres in vehicles other than signals after all.
% Note that mutual information is a measure between types, while semantic content is typically construed as a relation between tokens.
% To define one in terms of the other requires linking a type definition to a token definition.
% Skyrms does this by defining a specific formal object, what Isaac calls an s-vector, that defines a property of a token vehicle and that bears a clear formal relationship to mutual information.
% [REST OF THIS PARA: Why I disagree with the Skyrms-Isaac approach; why their project is quite different from naturalist representation]

% \citet[SECTION]{shea2018representation} outlines a set of extra conditions that a vehicle must meet in order to be considered a representation.
% \citet[$\S$4]{shea2018representation} carefully distinguishes token definitions from type definitions.
% [REST OF THIS PARA: carefully give Shea's definition and distinguish his goals from those of Mart\'{i}nez]

% \citet{lean2014shannon} and \citet[CHAPTER]{hutto2017evolving} accept that correlations between signalling vehicles and signifieds can do explanatory work in the functional sciences, but deny that functional correlations amount to semantic content.
% [REST OF THIS PARA: concede that teleosemantics uses `semantic content' in a very liberal sense; tie back to the extreme liberality of Skyrms/Isaac. Upshot: perhaps we are ALL running different projects and using the term `semantic content' in different ways.]

It must be noted that the four kinds of projects characterised above -- the one I endorse, plus the three motivated by an assumption about information theory that I reject -- could be made consistent with each other if only proponents agreed they meant different things by `semantic content'.
The Skyrms-Isaac project is to assign to vehicles a formal object capturing their correlational relationships.
Nobody thinks this is a bad idea; the controversial claim is that the property described by this object is \textit{the same property} that philosophers seek under the name `semantic content'.
The teleosemantic project is to define explanatorily relevant differences between signals and cues, rooted in the fact that signals have functions and cues do not.
Again, presumably all philosophers of science would find this a valuable project; the controversial part is the assertion that the property that distinguishes signals is \textit{the same property} that all other philosophers have been calling `semantic content'.
\citet{hutto2013radicalizing} are presumably correct that the set of properties characterising human socio-linguistic practices is different from the set of properties characterising biological signals; the question is whether those sets overlap and, if they do, whether one of the properties within that overlap is \textit{the same property} that other philosophers call `semantic content'.
A more cautious way of proceeding is demonstrated by \citet{shea2018representation}, who defines representational content in a manner that does specific work in cognitive science.
Shea explicitly disavows the claim that this notion should extend to other philosophical domains.
Since much of the work that goes on under the rubric of figuring out what semantic content `really is' ends up delineating explanatorily interesting differences in the ways that vehicles can bear exploitable relations to the world, headway can perhaps be made by focusing more on the distinctions and less on their labels.
All that being said, I reiterate that the assumption typically shared by scholars mentioned above is something I reject, and \textit{that} is not a verbal dispute -- however the debate over `semantic content' culminates.

In sum, scepticism about the relevance of communication theory for naturalist representation results from two illicit moves: first, from \ami{} to \ait{} via the term `Shannon information'; second, from \ait{} to \act{} via treating the two disciplines as identical.
The next section shows that the resultant claim about communication theory is simply false.
Communication theory can and does distinguish signals and cues.


%%%%%%
%%%%%%
%%%%%%
%%%%%%
\subsection{\act{} is false}

Whereas information theory is a collection of mathematical tools with wide application across the sciences, communication theory is an engineering discipline with the specific goal of designing efficient signalling techniques.
The vehicles transmitted in the central model are definitionally signals, not cues.
The sender produces the vehicle in accordance with an encoding scheme shared with the receiver.
This definition of `signal' is shared by several sciences \citep{shannon1948mathematicalc,maynardsmith2003animal} and philosophical accounts \citep[$\S$6]{millikan2004varieties} \citep{bergstrom2011transmission}.
Some sciences (especially historical sciences) use the word `signal' when referring to cues, but this is a difference in terminology and not a substantive dispute over definitions.

Not only does the formal apparatus of communication theory distinguish signals and cues, its theorems distinguish them too.
Consider, on the one hand, the \textbf{second theorem} and \textbf{third theorem} of communication theory, which require that the vehicles they address be signals, and on the other hand \textbf{Kelly's theorem}, which requires only that the vehicle be a cue.
We shall introduce them in turn.

The second theorem of communication theory (sometimes called the noisy channel theorem) determines how accurately a receiver is able to reconstruct the source string thanks to the sender's encoding scheme.
It is assumed that the channel over which signals are transmitted inserts noise into the signal.
Better encodings combat noise by building redundancy into the signal, enabling the receiver to more accurately reconstruct the source.
The theorem answers a question about receiver performance by attending to the sender's design of the vehicle: different encoding schemes would yield different performance levels.
The vehicle mentioned in this theorem is definitionally a signal.

The third theorem of communication theory (also known as the rate-distortion theorem) addresses a similar question, this time with the added benefit that the receiver need not achieve perfect performance.
Suppose for example that the receiver only needs to correctly reconstruct four out of every five outcomes produced by the source.
The third theorem states that it is possible to determine the minimum transmission rate that the sender must ensure in order for the receiver to perform at the specified level.
Again, the theorem assumes that the transmission rate is tunable by the sender's choice of encoding scheme.
By invoking a vehicle whose form can be adapted to performance specifications, both the second and third theorems employ a concept of signal rather than cue.

Kelly's theorem, by contrast, concerns the performance of a receiver conditioning its behaviour on a vehicle that bears a fixed level of mutual information with a success-relevant distal state \citep{kelly1956new}.
The theorem states that mutual information is an upper bound on the performance improvement a receiver can enjoy by using the cue rather than not using it.
Because the emphasis is on receiver behaviour, and the mutual information is held fixed, the vehicle is conceptualised as a cue.
The theorem has been applied in evolutionary biology \citep{donaldson-matasci2010fitness} where again the vehicle in question is treated as an environmental cue rather than a signal.

Of course, nothing prevents us applying Kelly's theorem to signals too.
My claim is not that theorems about cues do not apply to signals, but that theorems about signals do not apply to cues.
There is an asymmetry in the definition of signals and cues, and the two kinds of theorem are asymmetric in a way that reflects that.
Any vehicle can be treated as a cue simply by failing to specify whether or not it was designed for communicative use.
That is what Kelly's theorem does, in the guise of keeping the mutual information fixed and asking how the receiver can make use of it.
Kelly's prose implies that the vehicle in question is a signal, but on closer inspection the theorem does not require it to be one.
The second and third theorems by contrast require that their vehicles be signals, because they assure a level of functional performance that is only available when the sender tunes the vehicle's production to the features of the channel.
By definition, the second and third theorems cannot be applied to cues.

Furthermore, these points are reflected in the mathematical formalism of each theorem.
The second theorem discusses maximising the transmission rate of a channel by changing the encoding scheme, which changes the distribution of input symbols.
This can be written formally as $\max_{p(x)}I(X;Y)$.
In this set-up, $X$ and $Y$ are either end of a signalling channel.
In contrast, Kelly's theorem keeps $I(X;Y)$ fixed.
There is no $\max_{p(x)}I(X;Y)$; in this set-up, $Y$ is a cue for $X$.
As we have seen, measuring the mutual information between $X$ and $Y$ does not tell you whether one is a signal of the other.
And yet there exists formalism, in this case $\max_{p(x)}I(X;Y)$, that does distinguish whether a vehicle is being treated as a signal rather than a cue.

In sum, communication theory distinguishes signals from cues both by providing the means to define signals and by employing theorems that require the vehicles in question be signals.
The fact that communication theory also contains theorems like Kelly's whose vehicles need only be cues starkens the point.

%%%%%%%%%%%
%%%%%%%%%%%
%%%%%%%%%%%
%%%%%%%%%%%
\subsection{A sceptical riposte: channel capacity}

Alarm bells ring in sceptical ears: I just talked about the second theorem, and how its use of $\max_{p(x)}I(X;Y)$ confirms its vehicles are signals.
But the sceptic well knows that $\max_{p(x)}I(X;Y)$ is the \textbf{capacity} of the channel defined by $p(y|x)$.
The capacity is defined only in terms of the signal before noise ($X$) and the signal after noise ($Y$); it is oblivious to the source and target strings -- oblivious to what the signal is actually about.
If the second theorem and the channel capacity it defines are agnostic to the content of signals, is not the sceptic justified in asserting that the definition of semantic content must be found elsewhere than communication theory?
Dennett seems to be making this point when he distinguishes between the capacity and content of ``information-transmission and information-storage vehicles'' \citep[344]{dennett1983intentional}, and the present author has heard it from philosophers in personal communication too.

\begin{itemize}
    \item My response: It is true that \cia{}, but that does not affect any of the points I make above. You would have to establish that communication theory can ONLY measure channel capacity. That does indeed seem to be what Dennett implies, but if he really means it, he is wrong.
\end{itemize}

To summarise the entire section, a true premise about the broad application of mutual information in science entails nothing about the relevance of communication theory for naturalist approaches to content.
