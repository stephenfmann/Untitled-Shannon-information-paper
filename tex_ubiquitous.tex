\section{Agnostic information}\label{sec:agnostic}

The second route to scepticism about the relevance of communication theory for naturalist representation begins with the claim that mathematical measures defined within communication theory cannot distinguish between representations and non-representations.
In other words, information is agnostic to representational status.
In this section I argue that although this is true of certain mathematical functions like mutual information, communication theory has many more tools at its disposal.
Communication theory does distinguish between signals and non-signals -- it must do in order for its theorems to have sense.
The theory is fundamentally about the costs and benefits of representation and how to trade them off judiciously.
Philosophers undersell the resources available to communication theory by focusing solely on a small set of measures.

\subsection{How scientists use information theory}\label{subsec:scientists}

Around the same time Bar-Hillel \& Carnap were discussing the relevance of Shannon's work for philosophy, scientists began to notice that his mathematical tools were of use beyond the context of communications engineering.
For example, entropy, originally devised as a measure of how many code symbols would be needed to represent a sequence of source outcomes, was given a rather more general interpretation as a measure of the uncertainty associated with a probability space.
Intuitively, when entropy is high, an observer is less sure which outcome will occur, so entropy seems a reasonable measure of uncertainty.
Further uses can be derived from this more general interpretation.
Entropy has been used in ecology as a measure of diversity: when we treat the proportions of different species in an ecological population as probabilities, the entropy of that population measures our uncertainty about which species would be observed if we sampled randomly from the population.
This intuitively captures an aspect of the population diversity \citep{margalef1957information}.

Shifts in the interpretation of entropy and other measures accompanied the emergence of the term \textbf{information theory} to describe Shannon's mathematical tools and their more general application across the sciences.
Today, information theory comprises a set of concepts and measures common to many mathematical and scientific disciplines (see figure \ref{fig:info_theory}).
For better or worse, it has become customary for philosophers to use the term `information theory' to describe both these more general applications and the original context of communications engineering in which they were devised.
Since the claims I make below depend on this distinction being upheld, I will continue to use `information theory' and `communication theory' non-synonymously.

\input{fig_info_theory}

In the context of information theory, informational measures are often treated as statistical tools.
Perhaps the most well-known is \textbf{mutual information}, typically interpreted as a measure of the strength of correlation between two variables.
Mutual information has been employed in a diverse range of sciences, including:

\begin{itemize}
    \item Behavioural ecology, to measure the correlation between the honeybee waggle dance and the location of food sources \citep{haldane1954statistical}
    \item Molecular biology, to measure the correlation between inputs and outputs of a quorum-sensing bacterium \citep{mehta2009information}
    \item Evolutionary biology, to show that the correlation between an environmental cue and a fitness-relevant state of affairs is an upper bound on the increased growth rate of an organism conditioning its behaviour on the cue \citep{donaldson-matasci2010fitness}
    \item Cosmology, to measure the correlation between galaxies' internal morphology and their local environments \citep{pandey2017how}
    \item Linguistics, to measure the co-occurrence of words in a corpus \citep[$\S$4]{hunston2002corpora}
    \item Neuroscience, to measure the correlation between neural firings and environmental states \citep[][and references therein]{rathkopf2017neural}
\end{itemize}

% \noindent That many things correlate in the natural world is a useful fact for science to exploit.
% Correlations increase the opportunity to discover features about a target phenomenon.
% If the target is difficult to observe, it might more easily be studied by observing something that correlates with it. 
% Furthermore, correlations play a special role in the functional sciences.
% Biological and cognitive agents can condition their behaviour on cues and signals that correlate with success-relevant states of affairs, and scientists are therefore interested in how correlations are exploited in general.
% Mutual information and related measures are some of the most common tools employed in these investigations.

\noindent The breadth of application of mutual information is at the heart of a second source of scepticism about the relevance of communication theory for naturalist representation.
Scholars usually move from a premise about mutual information to a conclusion about information theory as a whole -- which is then seen as encompassing communication theory.
Because philosophers are rarely explicit about the dialectical moves required to reach this conclusion, in the next few subsections I have tried to tease apart the intended argument in order to refute it.
As a result, my discussion focuses on one possible way of moving from claims about mutual information to scepticism about the relevance of communication theory.
There may be other, better arguments that my account does not affect.

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\subsection{Mutual information cannot distinguish signals and cues}

Let us begin with a premise that sceptics and optimists alike should agree on: mutual information cannot distinguish between \textbf{signals} and \textbf{cues}.

Originating in behavioural ecology, the signal/cue distinction highlights the fact that some informational vehicles have the function to provide the information they do, whereas some are `accidentally' informational, used opportunistically by their receivers \citep[$\S$1.2]{maynardsmith2003animal}.
Vehicles selected to serve a communicative role are called signals, while vehicles that fortuitously provide information are called cues.
(In philosophy the term `natural sign' is sometimes used; I am here using `cue' to cover all cases described by that term.)
The waggle dance is a signal because it evolved in the honeybee lineage to serve as an informational vehicle that enables workers to enjoy greater success at foraging or nest-finding \citep{gould1975honey,riley2005flight}.
In contrast, bees' use of the position of the sun in the sky to navigate is a cue, because the sun's location is not an outcome of a process of selection that jointly produced both it and the bees' navigational behaviour.

Mutual information quantifies the strength of a correlation no matter whether its vehicles are signals or cues.
That a vehicle has positive mutual information with an environmental state does not determine whether that vehicle is a cue or a signal.
The sheer variety of scientific contexts in which mutual information is used emphasises this point.
While the correlation between the waggle dance and food locations is due to the fact that the waggle dance is a signal, the correlation between galaxies' morphology and their local environment clearly is not.
The vehicle in the evolutionary model of \citet{donaldson-matasci2010fitness} is definitionally a cue.
The co-occurrence of words in a corpus is not itself a signal, though the words themselves are representations (or at least combine to produce representations).
Without further detail, it is not clear whether the output of a quorum-sensing bacterium counts as a signal of its input; nonetheless, the mutual information between the two can be measured.
Neural firings are sometimes claimed to be representations, but simply measuring the correlation between them and environmental states is not sufficient to establish this \citep{rathkopf2017neural}.

From the mere fact that two things bear a correlational relationship, no conclusion can be drawn about whether one is a signal of the other.
% In other words, one cannot move from a claim about mutual information to a claim about signalhood.
For the avoidance of doubt, I agree with this point, and suggest encapsulating it as follows:

\begin{myquote}
{\sc Agnostic Mutual Information}: Mutual information cannot distinguish signals and cues.
\end{myquote}

\noindent The path to scepticism I want to explore is the move from {\sc Agnostic Mutual Information} to one or both of the following claims:

\begin{myquote}
{\sc Agnostic Information Theory}: Information theory cannot distinguish signals and cues.
\end{myquote}

\begin{myquote}
{\sc Agnostic Communication Theory}: Communication theory cannot distinguish signals and cues.
\end{myquote}

\noindent The latter claim would certainly challenge the relevance of communication theory for theories of representation.
If communication theory cannot distinguish signals and cues, then there is little hope of it distinguishing any of the more sophisticated kinds of representation of interest to cognitive scientists and philosophers; if it cannot distinguish them, it cannot say anything philosophically interesting about them.

I am going to argue firstly that {\sc Agnostic Mutual Information} does not entail {\sc Agnostic Communication theory}, and secondly that {\sc Agnostic Communication Theory} is false.
Of course, the second argument would immediately entail the first (because we are treating {\sc Agnostic Mutual Information} as uncontroversially true), but in laying out the first argument we can explore the intermediate claim {\sc Agnostic Information Theory}.
This allows us to describe the contemporary philosophical landscape in relation to the positive account behind my negative arguments.
The two arguments commence in the following two subsections.

%%%%%%
%%%%%%
%%%%%%
%%%%%%
\subsection{\sloppy {\sc Agnostic Mutual Information} does not entail {\sc Agnostic Communication theory}}

Let me lay out my suspicions clearly: I suspect that philosophers move from {\sc Agnostic Mutual Information} to the intermediate claim {\sc Agnostic Information Theory} by employing the term `Shannon information' to mean both `mutual information' and `all of the tools and concepts available to information theory'.
I further suspect that philosophers then move from {\sc Agnostic Information Theory} to {\sc Agnostic Communication Theory} by treating `information theory' and `communication theory' as synonymous.
These moves are almost always made implicitly, and almost always by invoking the term `Shannon information'.

`Shannon information' is given diverse definitions in philosophy (table \ref{tab:shannon}).
By invoking it, demonstrably true claims about mutual information can be interpreted as unsupported claims about information theory as a whole.
[REMAINDER OF THIS PARA: textual evidence that people junk all of info theory based on claims about mutual info]


\input{tab_shannon}

%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%
% \subsection{\sloppy Philosophers diverge on the conclusions drawn from {\sc Agnostic Information Theory}}

Although {\sc Agnostic Information Theory} is generally agreed upon, different authors draw very different conclusions from it.
I want to mention three popular lines of thought in order to distinguish them from the account that this paper is ultimately designed to support:

\begin{itemize}
    \item Information theory cannot distinguish signals and cues, but since information theory is our best chance of naturalising semantic content, informational measures alone must define semantic content; therefore, even cues possess semantic content \citep{skyrms2010signals,isaac2018semantics}
    \item Information theory cannot distinguish signals and cues, but since information theory is our best chance of naturalising semantic content, extra conditions must be added to informational measures before they constitute semantic content \citep[$\S$3-4]{shea2018representation}\citep[pp. 6-9,34-36]{neander2017mark}
    \item Information theory cannot distinguish signals and cues, therefore informational measures are not the right way to naturalise semantic content \citep{lean2014shannon} \citep[$\S$4]{hutto2017evolving} 
\end{itemize}

\noindent In contrast, the view I proposed in the previous section becomes:

\begin{itemize}
    \item Regardless whether information theory can distinguish signals and cues, semantic content is defined by tools of communication theory other than informational measures.
\end{itemize}

\noindent The three views I am rejecting share a premise: that the only tools available to information theory are mutual information and related measures.
In effect, the sceptical claim is that all of the tools of information theory are insufficient to distinguish signals and cues.
This may be true depending on how the limits of information theory are drawn, but it leads to a false conclusion when information theory is conflated with communication theory.
[REMAINDER OF THIS PARA: textual evidence that philosophers ignore the distinction between information theory and communication theory]

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%% Expand on the above views something like as follows ONLY IF it seems necessary/interesting/useful for the paper as a whole.
%% Finish the paper without the following, then consider whether it is improved by adding it back in.

% I will briefly describe each in order to distinguish the view I am defending.

% \citet{skyrms2010signals} and \citet{isaac2018semantics} flat-footedly assert that mutual information just \textit{is} semantic content, and that it turns out content inheres in vehicles other than signals after all.
% Note that mutual information is a measure between types, while semantic content is typically construed as a relation between tokens.
% To define one in terms of the other requires linking a type definition to a token definition.
% Skyrms does this by defining a specific formal object, what Isaac calls an s-vector, that defines a property of a token vehicle and that bears a clear formal relationship to mutual information.
% [REST OF THIS PARA: Why I disagree with the Skyrms-Isaac approach; why their project is quite different from naturalist representation]

% \citet[SECTION]{shea2018representation} outlines a set of extra conditions that a vehicle must meet in order to be considered a representation.
% \citet[$\S$4]{shea2018representation} carefully distinguishes token definitions from type definitions.
% [REST OF THIS PARA: carefully give Shea's definition and distinguish his goals from those of Mart\'{i}nez]

% \citet{lean2014shannon} and \citet[CHAPTER]{hutto2017evolving} accept that correlations between signalling vehicles and signifieds can do explanatory work in the functional sciences, but deny that functional correlations amount to semantic content.
% [REST OF THIS PARA: concede that teleosemantics uses `semantic content' in a very liberal sense; tie back to the extreme liberality of Skyrms/Isaac. Upshot: perhaps we are ALL running different projects and using the term `semantic content' in different ways.]

In sum, scepticism about the relevance of communication theory for naturalist representation results from two illicit moves: first, from {\sc Agnostic Mutual Information} to {\sc Agnostic Information Theory} via the term `Shannon information'; second, from {\sc Agnostic Information Theory} to {\sc Agnostic Communication Theory} via treating the two disciplines as identical.
The next section shows that the resultant claim about communication theory is simply false.
Communication theory can and does distinguish signals and cues.


%%%%%%
%%%%%%
%%%%%%
%%%%%%
\subsection{{\sc Agnostic Communication Theory} is false}

Whereas information theory is a collection of mathematical tools with wide application across the sciences, communication theory is an engineering discipline with the specific goal of designing efficient signalling techniques.
The vehicles transmitted in the central model are definitionally signals, not cues.
The sender produces the vehicle in accordance with an encoding scheme shared with the receiver.
This definition of `signal' is shared by several sciences \citep{shannon1948mathematicala,maynardsmith2003animal} and philosophical accounts \citep[$\S$6]{millikan2004varieties} \citep{bergstrom2011transmission}.

Not only does the formal apparatus of communication theory distinguish signals and cues, its theorems distinguish them too.
Consider, on the one hand, the \textbf{second theorem} and \textbf{third theorem} of communication theory, which require that the vehicles they address be signals, and on the other hand \textbf{Kelly's theorem}, which requires only that the vehicle be a cue.
We shall introduce them in turn.

The second theorem of communication theory (sometimes called the noisy channel theorem) determines what level of performance a receiver is able to achieve thanks to the sender's encoding scheme.
It is assumed that the channel over which their signals are transmitted and received inserts noise into the signal.
The theorem answers a question about receiver performance by attending to the sender's design of the vehicle: different encoding schemes would yield different performance levels.
The vehicle mentioned in this theorem is definitionally a signal.

The third theorem of communication theory (also known as the rate-distortion theorem) addresses a similar question, this time with the added benefit that the receiver need not achieve perfect performance.
Suppose for example that the receiver only needs to reconstruct four out of every five outcomes produced by the source.
The third theorem states that it is possible to determine the minimum transmission rate that the sender must ensure in order for the receiver to perform at the specified level.
Again, the theorem assumes that the transmission rate is tunable by the sender's choice of encoding scheme.
By invoking a vehicle whose form can be adapted to performance specifications, both the second and third theorems employ a concept of signal rather than cue.

Kelly's theorem, by contrast, concerns the performance of a receiver conditioning its behaviour on a vehicle that bears a fixed level of mutual information with a signified \citep{kelly1956new}.
The theorem states that mutual information is an upper bound on the performance improvement a receiver can enjoy by using the cue rather than not using it.
Because the emphasis is on receiver behaviour, and the mutual information is held fixed, the vehicle is conceptualised as a cue.
The theorem has been applied in evolutionary biology \citep{donaldson-matasci2010fitness} where again the vehicle in question is treated as an environmental cue rather than a signal.

Of course, nothing prevents us applying Kelly's theorem to signals too.
My claim is not that theorems about cues do not apply to signals, but that theorems about signals do not apply to cues.
There is an asymmetry in the definition of signals and cues, and the two kinds of theorem are asymmetric in a way that reflects that.
Any vehicle can be treated as a cue simply by failing to specify whether or not it was designed for communicative use.
That is what Kelly's theorem does, in the guise of keeping the mutual information fixed and asking how the receiver can make use of it.
Kelly's prose implies that the vehicle in question is a signal, but on closer inspection the theorem does not require it to be one.
The second and third theorems by contrast require that their vehicles be signals, because they assure a level of functional performance that is only available when the sender tunes the vehicle's production to the features of the channel.
By definition, the second and third theorems cannot be applied to cues.

Furthermore, these points are reflected in the mathematical formalism of each theorem.
The second theorem discusses maximising the transmission rate of a channel by changing the encoding scheme, which changes the distribution of input symbols.
This can be written formally as $\max_{p(x)}I(X;Y)$.
In this set-up, $X$ and $Y$ are either end of a signalling channel.
In contrast, Kelly's theorem keeps $I(X;Y)$ fixed.
There is no $\max_{p(x)}I(X;Y)$; in this set-up, $Y$ is a cue for $X$.
As we have seen, measuring the mutual information between $X$ and $Y$ does not tell you whether one is a signal of the other.
And yet there exists formalism, in this case $\max_{p(x)}I(X;Y)$, that does distinguish whether a vehicle is being treated as a signal rather than a cue.

In sum, communication theory distinguishes signals from cues both by providing the means to define signals and by employing theorems that require the vehicles in question be signals.
The fact that communication theory also contains theorems like Kelly's whose vehicles need only be cues makes this point all the more stark.

%%%%%%%%%%%
%%%%%%%%%%%
%%%%%%%%%%%
%%%%%%%%%%%
\subsection{A sceptical riposte: channel capacity}

\begin{itemize}
    \item Sceptic: We can measure the capacity of a channel without knowing what its signals are about
    \item My response: That is true, but does not affect any of the points I make above. You would have to establish that communication theory can ONLY measure channel capacity. That does indeed seem to be what Dennett implies, but if he really means it, he is wrong.
\end{itemize}

To summarise the entire section, true premises about the broad application of mutual information in science entail nothing about the relevance of communication theory for naturalist approaches to content.
