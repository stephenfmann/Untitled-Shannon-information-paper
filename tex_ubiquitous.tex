\section{Ubquitous information}\label{sec:ubiquitous}

\subsection{How scientists use information theory}\label{subsec:scientists}

Around the same time that Bar-Hillel \& Carnap were discussing the relevance of Shannon's work for philosophy, scientists noticed that his mathematical tools were of use beyond the context of communication.
Entropy, originally devised as a measure of how many code symbols would be needed to represent the outcomes of a source process, was given a rather more general interpretation as a measure of the uncertainty associated with a probability space.
Intuitively, when entropy is high, an observer is less sure which outcome will occur, so entropy seems a reasonable measure of uncertainty.
Further uses can be derived from this more general interpretation.
For example, entropy has been used in ecology as a measure of diversity \citep{margalef1957information}.
When we treat the proportions of different species in an ecological population as probabilities, the entropy of that population captures the uncertainty about which species would be observed if one sampled randomly from the population.
This intuitively captures an aspect of the population diversity.

Shifts in the interpretation of entropy and other measures accompanied the emergence of the term \textbf{information theory} to describe the set of mathematical tools and their more general application across the sciences (see figure \ref{fig:info_theory}).
Despite the distinctions implied by the figure, it is nowadays customary for philosophers to use the term `information theory' to describe both these more general applications and the original context of communications engineering in which they were devised.
I will continue to distinguish communication theory from information theory, because the domain of application of the measures affects the truth values of philosophical claims made about them.

\input{fig_info_theory}

Treated as tools of information theory, informational measures are statistical tools.
Perhaps the most well-known is \textbf{mutual information}, typically interpreted as a measure of the strength of correlation between two variables.
Mutual information has been employed in a diverse range of sciences, including:

\begin{itemize}
    \item Behavioural ecology, to measure the correlation between the honey bee waggle dance and the location of food sources \citep{haldane1954statistical}
    \item Molecular biology, to measure the correlation between inputs and outputs of a quorum-sensing bacterium \citep{mehta2009information}
    \item Evolutionary biology, to measure the growth rate of an organism conditioning its behaviour on an environmental cue \citep{donaldson-matasci2010fitness}
    \item Cosmology, to measure the correlation between galaxies' internal morphology and their local environments \citep{pandey2017how}
    \item Linguistics, to measure the co-occurrence of words in a corpus \citep[$\S$4]{hunston2002corpora}
    \item Neuroscience, to measure the correlation between neural firings and environmental states \citep[][and references therein]{rathkopf2017neural}
\end{itemize}

\noindent That many things correlate in the natural world is a useful fact for science to exploit.
Correlations increase the opportunity to discover features about a target phenomenon.
If the target is difficult to observe, it might more easily be studied by observing something that correlates with it. 
Furthermore, correlations play a special role in the functional sciences.
Biological and cognitive agents can themselves make use of correlations.
Scientists are therefore interested in how correlations are exploited by agents other than scientists themselves.

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\subsection{Ubiquity of mutual information generates scepticism about the relevance of information theory}

The wide applicability of mutual information generates scepticism about the relevance of information theory for accounts of representation.
To demonstrate how, it will help to introduce the \textbf{signal/cue distinction}.
Originating in behavioural ecology, the distinction highlights the fact that some informational vehicles have an evolutionary function to provide the information they do, whereas some are `accidentally' informational, used opportunistically by their receivers \citep[PAGES]{maynardsmith2003animal}.
Vehicles selected to serve a communicative role are called signals, while vehicles that fortuitously provide information are called cues.
(In philosophy the term `natural sign' is sometimes used; I am here using `cue' to cover all cases described by that term.)
For example, the waggle dance is a signal because it evolved to serve as an informational behaviour that enable workers to enjoy greater success at foraging or nest-finding.
Bees additionally can use the position of the sun in the sky to navigate.
This is a cue, because the sun's location is not an outcome of a process of selection that jointly produced both it and the bees' navigational behaviour.

Mutual information quantifies the strength of a correlation no matter whether its vehicles are signals or cues, and no matter who or what is exploiting that correlation.
As a result, that a vehicle has positive mutual information with an environmental state does not determine whether that vehicle is a cue or a signal.
This fact leads to the second line of scepticism against the relevance of information theory for naturalist accounts of content.

%%%%%%
%%%%%%
%%%%%%
%%%%%%
\subsection{The argument for insufficiency of information theory}

The argument is weaker than that discussed in the first part.
Whereas Shannon's Warning was thought to make information theory completely irrelevant to theories of content, the ubiquity of information only constrains information theory's relevance.
Nonetheless, it is a harsh constraint.
The argument is as follows: because mutual information cannot distinguish signals and cues, information theory cannot distinguish signals and cues.

Sceptics then diverge on the consequences of this fact.
\citet{skyrms2010signals} and \citet{isaac2018semantics} flat-footedly assert that mutual information just \textit{is} semantic content, and that it turns out content inheres in vehicles other than signals after all.
\citet[SECTION]{shea2018representation} outlines a set of extra conditions that a vehicle must meet in order to be considered a representation. 
\citet{lean2014shannon} and \citet[CHAPTER]{hutto2017evolving} accept that correlations between signalling vehicles and signifieds can do explanatory work in the functional sciences, but deny that functional correlations amount to semantic content.

Although the conclusions drawn vary widely, all these scholars share a premise: that the only tools available to information theory are mutual information and related measures.
In effect, the sceptical claim is that all of the tools of information theory are insufficient to distinguish signals and cues.

I wish to argue that it is irrelevant whether or not information theory has the tools to distinguish signals and cues.
The theory that should be of interest to philosophers is communication theory.
Whereas information theory is a collection of mathematical tools with wide application across the sciences, communication theory is an engineering discipline with the specific goal of designing efficient signalling techniques.
Sufficiently generalised, the formal concept of a signal defined in that theory captures many of the features of representations of interest to science -- including their content.
The role of mutual information is less prominent than has been supposed by philosophers.

%%%%%%
%%%%%%
%%%%%%
%%%%%%
\subsection{Communication theory distinguishes signals and cues}

The vehicles transmitted in the central model are definitionally signals, not cues.
The sender produces the vehicle in accordance with an encoding scheme shared with the receiver.
This definition of `signal' is shared by several sciences \citep{shannon1948mathematicala,maynardsmith2003animal} and philosophical accounts \citep[$\S$6]{millikan2004varieties} \citep{bergstrom2011transmission}.

Not only does the formal apparatus distinguish signals and cues, the theorems of communication theory distinguish them too.
Consider, on the one hand, the \textbf{second theorem} and \textbf{third theorem} of communication theory, which require that the vehicles they address be signals, and on the other hand \textbf{Kelly's theorem}, which requires only that the vehicle be a cue.
The second theorem (sometimes called the noisy channel theorem) determines what level of performance the receiver is able to achieve thanks to the sender's encoding scheme, in the face of a channel that inserts noise into the vehicle.
The theorem answers a question about receiver performance by attending to the sender's design of the vehicle: different encoding schemes would yield different performance levels.
The vehicle mentioned in this theorem is definitionally and necessarily a signal.
The third theorem addresses a similar question, this time with the added benefit that the receiver need not achieve perfect performance.
Suppose for example that the receiver needs to reconstruct four out of every five source events.
The third theorem states that it is possible to determine the minimum transmission rate that the sender must ensure in order for the receiver to perform at the specified level.
Again, the theorem assumes that the transmission rate is tunable by the sender's choice of encoding scheme.
By invoking a vehicle whose form can be adapted to performance specifications, both the second and third theorems employ a concept of signals rather than cues.

Kelly's theorem, by contrast, concerns the performance of a receiver given a vehicle that bears a fixed level of mutual information with a signified \citep{kelly1956new}.
The theorem states that the mutual information is an upper bound on the performance \textit{improvement} a receiver can enjoy by using the cue rather than not using it.
Because the emphasis is on receiver behaviour, and the mutual information is not a quantity that can change, the vehicle is conceptualised as a cue.
The theorem has been applied in evolutionary biology \citep{donaldson-matasci2010fitness} where again it treats environmental cues rather than signals.

Of course, nothing prevents us applying Kelly's theorem to a signal too.
The claim is not that theorems about cues do not apply to signals, but that theorems about signals do not apply to cues.
There is an asymmetry in the definition of signals and cues.
The two kinds of theorem are asymmetric in a way that reflects that.
Any vehicle can be treated as a cue simply by ignoring aspects of its production design.
That is what Kelly's theorem does, in the guise of keeping the mutual information fixed and asking how the receiver can make use of it.
Shannon's theorems require that vehicles be signals because they assure a level of functional performance that is only available when the sender is able to tune the signal's production to both the features of the channel and the receiver's needs.

%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
\subsection{Signal content is not determined by mutual information}

As we saw earlier, communication theorists use a pretheoretic notion of representation.
The content of a signal is the source string it encodes.
This result tallies with teleosemantics.

In this light, it is true but irrelevant that both signals and cues bear mutual information with their signifieds. 
The content of a signal is not determined by reference to mutual information at all!
It is determined by the encoding scheme by which it was designed.
While it is true that the signalling systems we usually discuss do indeed include signals that bear mutual information with source events, it is not in virtue of this that an individual signal has the content it does.
A signal's content is instead determined by the encoding scheme in accordance with which the sender produced it.

Another reason why signal content should not, is not, and perhaps cannot be determined by mutual information is that mutual information is a measure between types, while semantic content is a relation between tokens.



%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
\subsection{Extending communication theory to cognitive science}

The point of this section -- the point of this paper -- is to remove obstacles to the application of communication theory in cognitive science.
A few words are in order addressing that application.
What do we actually gain by studying the brain in the light of an engineering discipline?

\begin{itemize}
    \item Mart\'{i}nez's basic claims
    \item Examples of comm theory concepts in cog sci proper e.g. Sims on rate-distortion theory
    \item The general lesson: communication theory is about the cost of representation, and how those costs trade off against the benefits of representation. There is no reason why such a theory would be limited to binary symbols in digital channels. Its claims are universal.
    \item Future: what we really need is a general theory of function.
\end{itemize}
