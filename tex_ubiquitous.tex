\section{Ubiquitous information}\label{sec:ubiquitous}

\subsection{How scientists use information theory}\label{subsec:scientists}

Around the same time that Bar-Hillel \& Carnap were discussing the relevance of Shannon's work for philosophy, scientists noticed that his mathematical tools were of use beyond the context of communications engineering.
Entropy, originally devised as a measure of how many code symbols would be needed to represent the outcomes of a source process, was given a rather more general interpretation as a measure of the uncertainty associated with a probability space.
Intuitively, when entropy is high, an observer is less sure which outcome will occur, so entropy seems a reasonable measure of uncertainty.
Further uses can be derived from this more general interpretation.
For example, entropy has been used in ecology as a measure of diversity: when we treat the proportions of different species in an ecological population as probabilities, the entropy of that population measures uncertainty about which species would be observed if one sampled randomly from the population.
This intuitively captures an aspect of the population diversity \citep{margalef1957information}.

Shifts in the interpretation of entropy and other measures accompanied the emergence of the term \textbf{information theory} to describe this set of mathematical tools and their more general application across the sciences (see figure \ref{fig:info_theory}).
Despite the distinctions implied by the figure, it is nowadays customary for philosophers to use the term `information theory' to describe both these more general applications and the original context of communications engineering in which they were devised.
I will continue to distinguish communication theory from information theory, because the domain of application of the measures affects the truth values of philosophical claims made about them.

\input{fig_info_theory}

In the context of information theory, informational measures are often treated as statistical tools.
Perhaps the most well-known is \textbf{mutual information}, typically interpreted as a measure of the strength of correlation between two variables.
Mutual information has been employed in a diverse range of sciences, including:

\begin{itemize}
    \item Behavioural ecology, to measure the correlation between the honeybee waggle dance and the location of food sources \citep{haldane1954statistical}
    \item Molecular biology, to measure the correlation between inputs and outputs of a quorum-sensing bacterium \citep{mehta2009information}
    \item Evolutionary biology, to show that the correlation between an environmental cue and a fitness-relevant state of affairs is an upper bound on the increased growth rate of an organism conditioning its behaviour on the cue \citep{donaldson-matasci2010fitness}
    \item Cosmology, to measure the correlation between galaxies' internal morphology and their local environments \citep{pandey2017how}
    \item Linguistics, to measure the co-occurrence of words in a corpus \citep[$\S$4]{hunston2002corpora}
    \item Neuroscience, to measure the correlation between neural firings and environmental states \citep[][and references therein]{rathkopf2017neural}
\end{itemize}

\noindent That many things correlate in the natural world is a useful fact for science to exploit.
Correlations increase the opportunity to discover features about a target phenomenon.
If the target is difficult to observe, it might more easily be studied by observing something that correlates with it. 
Furthermore, correlations play a special role in the functional sciences.
Biological and cognitive agents can condition their behaviour on cues and signals that correlate with success-relevant states of affairs, and scientists are therefore interested in how correlations are exploited by agents other than scientists themselves.
Mutual information and related measures are some of the most common tools employed in these investigations.

%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\subsection{Mutual information cannot distinguish signals and cues}

The wide applicability of mutual information generates scepticism about the relevance of information theory for accounts of representation.
To demonstrate how, it will help to introduce one more distinction, between \textbf{signals} and \textbf{cues}.

Originating in behavioural ecology, the signal/cue distinction highlights the fact that some informational vehicles have an evolutionary function to provide the information they do, whereas some are `accidentally' informational, used opportunistically by their receivers \citep[$\S$1.2]{maynardsmith2003animal}.
Vehicles selected to serve a communicative role are called signals, while vehicles that fortuitously provide information are called cues.
(In philosophy the term `natural sign' is sometimes used; I am here using `cue' to cover all cases described by that term.)
For example, the waggle dance is a signal because it evolved in the honeybee lineage to serve as an informational vehicle that enables workers to enjoy greater success at foraging or nest-finding.
In contrast, bees' use of the position of the sun in the sky to navigate is a cue, because the sun's location is not an outcome of a process of selection that jointly produced both it and the bees' navigational behaviour.

Scepticism about the relevance of information theory for naturalist approaches to content gains traction when we note that mutual information quantifies the strength of a correlation no matter whether its vehicles are signals or cues.
That a vehicle has positive mutual information with an environmental state does not determine whether that vehicle is a cue or a signal.
The aforementioned uses of mutual information across the sciences emphasise this point.
While the correlation between the waggle dance and food locations is due to the fact that the waggle dance is a signal, the correlation between galaxies' morphology and their local environment is clearly not.
The vehicle in the evolutionary model of \citet{donaldson-matasci2010fitness} is definitionally a cue.
The co-occurrence of words in a corpus is not itself a signal, though the words themselves are representations (or at least combine to produce representations).
Without further detail, it is not clear whether the output of a quorum-sensing bacterium counts as a signal of its input; nonetheless, the mutual information between the two can be measured.

So there is no necessary connection between mutual information and signalhood.
From the mere fact that two things bear a correlational relationship, no conclusions whatever can be drawn about whether or not one is a signal of the other.
Many scholars derive from this fact a premise to the effect that information theory itself cannot distinguish signals and cues, though they build from this premise towards different conclusions.

% On the assumption that the naturalist wants to restrict attribution of semantic content at least to signals, mutual information bears no relevance for semantic content.\footnote{A prominent rejection of the assumption that signalhood is necessary for semantic content comes from the work of \citet{skyrms2010signals} and \citet{isaac2018semantics}. Both authors define semantic content in terms of mutual information and related mathematical objects. In Isaac's case, this is because he considers the term `semantics' to refer to the attribution of a formal object to each element in a system of signs, which enables him to define a `semantics' for cues. Because Isaac's goal is very different from that of teleosemantics, I treat him as solving a fundamentally different problem than the one we are concerned with here.}

%%%%%%
%%%%%%
%%%%%%
%%%%%%
\subsection{Diverse conclusions share the premise that information theory cannot distinguish signals and cues}

Several authors appear to move from the claim that mutual information cannot distinguish signals and cues to the claim that information theory cannot distinguish signals and cues.
How do they achieve this?
Typically the move is made by employing the term `Shannon information' to refer to both mutual information specifically and all the concepts and tools of information theory in general.
That this \textit{is} a move -- from a claim about mutual information to a claim about information theory -- is concealed by packaging it as a single statement about `Shannon information'.
The move is made harder to spot by the sheer diversity of definitions of `Shannon information' in the literature (table \ref{tab:shannon}).

The argument, in a nutshell, is that because mutual information cannot distinguish signals and cues, information theory cannot distinguish signals and cues.

Contemporary scholars then diverge on the conclusions they draw from the premise.
\citet{skyrms2010signals} and \citet{isaac2018semantics} flat-footedly assert that mutual information just \textit{is} semantic content, and that it turns out content inheres in vehicles other than signals after all.
\citet[SECTION]{shea2018representation} outlines a set of extra conditions that a vehicle must meet in order to be considered a representation. 
\citet{lean2014shannon} and \citet[CHAPTER]{hutto2017evolving} accept that correlations between signalling vehicles and signifieds can do explanatory work in the functional sciences, but deny that functional correlations amount to semantic content.

Although the conclusions drawn vary widely, all these scholars share a premise: that the only tools available to information theory are mutual information and related measures.
In effect, the sceptical claim is that all of the tools of information theory are insufficient to distinguish signals and cues.

I actually do not need to argue against this claim.
It is immaterial what distinctions information theory can or cannot draw.
What matters is that communication theory can distinguish signals and cues.
Scepticism about the relevance of communication theory for naturalist approaches to content results from two illicit moves: first, from a claim about mutual information to a claim about the whole of information theory via the term `Shannon information'; second, from the resultant claim about information theory to a claim about communication theory via treating the two disciplines as identical.
The next section shows that the resultant claim about communication theory is simply false; this section was intended just to highlight how many gaps there are in the reasoning along the way.

%% I'm putting the table here because it was causing a huge gap in the text.
\input{tab_shannon}

%%%%%%
%%%%%%
%%%%%%
%%%%%%
\subsection{Communication theory distinguishes signals and cues}

I wish to argue that it is irrelevant whether or not information theory has the tools to distinguish signals and cues.
The theory that should be of interest to philosophers is communication theory.
Whereas information theory is a collection of mathematical tools with wide application across the sciences, communication theory is an engineering discipline with the specific goal of designing efficient signalling techniques.
Sufficiently generalised, the formal concept of a signal defined in that theory captures many of the features of representations of interest to science -- including their content.
The role of mutual information is less prominent than has been supposed by philosophers.

The vehicles transmitted in the central model are definitionally signals, not cues.
The sender produces the vehicle in accordance with an encoding scheme shared with the receiver.
This definition of `signal' is shared by several sciences \citep{shannon1948mathematicala,maynardsmith2003animal} and philosophical accounts \citep[$\S$6]{millikan2004varieties} \citep{bergstrom2011transmission}.

Not only does the formal apparatus distinguish signals and cues, the theorems of communication theory distinguish them too.
Consider, on the one hand, the \textbf{second theorem} and \textbf{third theorem} of communication theory, which require that the vehicles they address be signals, and on the other hand \textbf{Kelly's theorem}, which requires only that the vehicle be a cue.
The second theorem (sometimes called the noisy channel theorem) determines what level of performance the receiver is able to achieve thanks to the sender's encoding scheme, in the face of a channel that inserts noise into the vehicle.
The theorem answers a question about receiver performance by attending to the sender's design of the vehicle: different encoding schemes would yield different performance levels.
The vehicle mentioned in this theorem is definitionally and necessarily a signal.
The third theorem addresses a similar question, this time with the added benefit that the receiver need not achieve perfect performance.
Suppose for example that the receiver needs to reconstruct four out of every five source events.
The third theorem states that it is possible to determine the minimum transmission rate that the sender must ensure in order for the receiver to perform at the specified level.
Again, the theorem assumes that the transmission rate is tunable by the sender's choice of encoding scheme.
By invoking a vehicle whose form can be adapted to performance specifications, both the second and third theorems employ a concept of signals rather than cues.

Kelly's theorem, by contrast, concerns the performance of a receiver given a vehicle that bears a fixed level of mutual information with a signified \citep{kelly1956new}.
The theorem states that the mutual information is an upper bound on the performance improvement a receiver can enjoy by using the cue rather than not using it.
Because the emphasis is on receiver behaviour, and the mutual information is held fixed, the vehicle is conceptualised as a cue.
The theorem has been applied in evolutionary biology \citep{donaldson-matasci2010fitness} where again the signs in question are treated as environmental cues rather than signals.

Of course, nothing prevents us applying Kelly's theorem to a signal too.
The claim is not that theorems about cues do not apply to signals, but that theorems about signals do not apply to cues.
There is an asymmetry in the definition of signals and cues, and the two kinds of theorem are asymmetric in a way that reflects that.
Any vehicle can be treated as a cue simply by failing to specify whether or not it was designed for communicative use.
That is what Kelly's theorem does, in the guise of keeping the mutual information fixed and asking how the receiver can make use of it.
In contrast, the second and third theorems require that vehicles be signals because they assure a level of functional performance that is only available when the sender is able to tune the signal's production to both the features of the channel and the receiver's needs.

In sum, communication theory distinguishes signals from cues both by providing the means to define signals and by employing theorems that require the vehicles in question be signals.

%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
%%%%%%%%%%%%%
\subsection{Signal content is not determined by mutual information}

What can the naturalist philosopher conclude from the fact that communication theory distinguishes signals and cues?

Before they conclude anything, they should note that nothing we have said so far implies that signal content is determined by mutual information.
Some contemporary treatments view content as being constructed from mutual information, typically by adding constraints such as that the vehicle has the function to carry it.
However, as we saw earlier, the pretheoretic notion of representation used in communication theory treats the content of a signal as the source string it encodes.
This result is given a theoretical underpinning by teleosemantics, and neither the pretheoretic definition nor the teleosemantic account treats mutual information as part of the definition of signal content.
It is the encoding scheme, not the correlation, that determines the content of a signal.

In this light, it is true but irrelevant that both signals and cues bear mutual information with their signifieds. 
The content of a signal is not determined by reference to mutual information at all!
It is determined by the encoding scheme by which it was designed.
While it is true that the signalling systems we usually discuss do indeed involve signals that bear mutual information with source events, it is not in virtue of this that an individual signal has the content it does.
A signal's content is instead determined by the encoding scheme in accordance with which the sender produced it.

Once we break the link between mutual information and semantic content we start to notice other differences between the two.
Mutual information is a measure between types, while semantic content is typically construed as a relation between tokens.

In sum, true premises about the broad application of mutual information in science entail nothing about the relevance of communication theory for naturalist approaches to content.


