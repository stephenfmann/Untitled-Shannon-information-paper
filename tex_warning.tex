%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%%%%%%%% Shannon's Warning %%%%%%%%%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Shannon's Warning}\label{sec:warning}

The first route to scepticism about the relevance of communication theory for naturalist representation begins with the claim that Claude Shannon, the founder of communication theory, warned that his theory had nothing to do with meaning.
In this section I argue that what Shannon actually said has been misconstrued by philosophers.
He was talking about a different aspect of his model, not signals themselves.
As it turns out, Shannon asserted a pretheoretic notion of content for communication-theoretic signals, which contemporary communication theorists still endorse, and which teleosemantics captures.

\subsection{The central model of communication theory}\label{subsec:central}

% Prior to the Second World War, communications engineering made progress through practice rather than theory.
% While technologies and coding techniques improved, theoretical background was sparse.
% Work by \citet{nyquist1924certain} and \citet{hartley1928transmission}, however, indicated that a generalised mathematical framework for communication was possible.
% In part due to the heavy cryptographic and communicative demands of the war effort, Claude Shannon came to devise the general theory we know today.

The heavy cryptographic and communicative demands of the Second World War led mathematicians and engineers, spearheaded by Claude Shannon, to develop the discipline known today as communication theory.
Published soon after the war's conclusion, the insights of Shannon's foundational text \parencite*{shannon1948mathematicalc} are predicated on a picture of communication we shall call the \textbf{central model} (figure \ref{fig:central}).
The central model construes the goal of communication as reproducing, at a target location, a symbol string produced at a spatiotemporally distant source.
The goal is achieved by \textbf{encoding} the source string, which means converting it into a sequence of physical events (typically electrical pulses) that can be transmitted as a signal from the source to the target.
At the target the signal is decoded.
Communication is deemed successful when the resultant target string is sufficiently similar to the source string.
How similar the two strings need to be to count as `sufficiently similar' will differ depending on the context.
What is important is that communication is pitched as a \textit{syntactic} enterprise: it is the transmission and reconstruction of symbol strings, not the conveyance of their meanings, that is in question.

\input{fig_central.tex}

For example, the lexicon from which the source string is constructed might be the set of symbols of the Latin alphabet $\{A,B,C...\}$ plus a full stop and a space.
The code lexicon might be the binary symbols $\{0,1\}$ that are instantiated by electrical on/off pulses.
Given an encoding scheme, any string of Latin symbols can be converted into a sequence of 0s and 1s, and thus transmitted as a signal across a wire.
The decoder has a duplicate set of Latin symbols from which it must pick out the right symbols in the right order; the signal enables it to perform this task successfully.

It is costly to transmit encoded symbols.
Electric wires require power and time to operate.
Communication theory can be thought of as a collection of tools and methods enabling an optimal trade-off between signalling effort and the benefits of accurate string reconstruction.
In the simple case of the central model, the most efficient coding schemes are those that use short sequences of 0s and 1s to represent highly probable source strings.
That is because minimising signalling effort means minimising the number of code symbols transmitted, on average.
Pairing probable source strings with short code strings -- short signals -- is the most efficient procedure.
Therefore, in order to devise a good code, you need to know the probabilities of each source string being produced.
Crucially, that is \textit{all} you need to know.
Whether or not source strings also carry natural language meaning is irrelevant to the problem of designing a code.
Shannon stated this clearly, as the next subsection details.

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Shannon's Warning}\label{subsec:warning}

Communication theory makes heavy use of the term `information'.
Shannon understood the semantic connotations of the term and took care to fend off misinterpretation.
In the introduction to the first of his foundational papers he writes:

\begin{myquote}
The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have \emph{meaning}; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem.
\par\hspace*{\fill}\citet[379]{shannon1948mathematicalc}, emphasis original
\end{myquote}

\noindent Clearly ``message'' in this context refers to a source string.
Shannon warns that the semantic properties of lexical elements do not affect the process of transmitting and reconstructing them.
To see why this is true, note that the strings of Latin symbols described in the above example need not form English-language words, nor words of any other language.
The problem of reconstructing those strings has a distinct mathematical sense, regardless of the strings' natural language implications.

In 1949 Shannon's papers were released in a single volume with prefatory remarks by Warren Weaver \citep{shannon1949mathematical}.
One of Weaver's comments expands on Shannon's earlier technical statement:

\begin{myquote}
In fact, two messages, one of which is heavily loaded with meaning and the other of which is pure nonsense, can be exactly equivalent, from the present viewpoint, as regards information. It is this, undoubtedly, that Shannon means when he says that ``the semantic aspects of communication are irrelevant to the engineering aspects.''
\par\hspace*{\fill}\citet[8]{shannon1949mathematical}
\end{myquote}

\noindent Weaver misquotes Shannon (``\emph{the} semantic aspects'' instead of ``\emph{these} semantic aspects''; ``the engineering \textit{aspects}'' instead of ``the engineering \textit{problem}'').
In context the mistake is insignificant, because the preceding sentences demonstrate that Weaver interprets the point accurately.
By `information' he means a certain property of source strings, defined as the optimal number of symbols in the corresponding code string.
This measure is more commonly known as \textbf{surprisal}, and is defined as $\log{\frac{1}{p(x)}}$ for a message $x$ that is produced at the source with probability $p(x)$.
It is clear that two messages -- two source strings -- can have the same surprisal, with one being a meaningful sentence of a natural language and the other being nonsense.
One need only define a source that produces a meaningful sentence and a nonsensical sentence with the same probability.

In context the misquote is unproblematic, but out of context Weaver can be read as stating what I will eventually deny: that the semantic properties of both the source string \textit{and the code string} are irrelevant to the well-functioning of engineered communication systems.
To dispel any doubt, I endorse Shannon's original claim.
I take it to be as follows:

\begin{myquote}
{\sc Shannon's Warning}: In the central model, once the statistical properties of source strings have been taken into account, the semantic properties of source strings are irrelevant to the engineering problem of communication.
\end{myquote}

\noindent 
% Symbols of a source lexicon consisting of Latin characters can be combined to produce messages that would be meaningful as strings of English.
% But such meanings are irrelevant to the problem of converting those strings into sequences of 0s and 1s from a code lexicon.
The meanings of source strings are not represented in the mathematics of communication theory.
{\sc Shannon's Warning} tells us that finding an efficient solution to the fundamental problem of communication requires knowing only the statistical properties of symbols in the source lexicon, not the meanings of strings constructed therefrom.

One might think that the meanings of source strings \emph{could} play a role in reconstructing them efficiently.
An intelligent observer receiving a noisy signal that decodes to {\sc SHALL I COMPARW TGEE TO A SUMNERS DAY} might be able to reconstruct the original Shakespearean line on the basis of its presumed meaning.
The point of {\sc Shannon's Warning} is not to rule this out, but to circumscribe the statistical aspects of the problem.
(In fact, it might not take much sophistication to design an error-correcting receiver that makes use of the fact that {\sc RW} rarely occurs in the messages it receives to correct {\sc COMPARW} to {\sc COMPARE} on purely statistical grounds.
Weaver's remarks include an extensive discussion of such issues \citep[$\S$2]{shannon1949mathematical}.)

Philosophers took heed of {\sc Shannon's Warning}.
But Weaver's misquote led to widespread misinterpretation.

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Philosophers' interpretations of the warning}\label{subsec:warningPhil}

Soon after Shannon's initial publications, \citet{bar-hillel1953semantic} set the standard for philosophical interpretation of communication theory:

\begin{myquote}
The Mathematical Theory of Communication, often referred to also as Theory (of Transmission) of Information, as practised nowadays, is not interested in the content of the symbols whose information it measures. The measures, as defined, for instance, by Shannon, have nothing to do with what these symbols symbolise, but only with the frequency of their occurrence.
\par\hspace*{\fill}\citet[147]{bar-hillel1953semantic}
\end{myquote}

\noindent Like Weaver, Bar-Hillel and Carnap likely understood {\sc Shannon's Warning} correctly; like Weaver, the words they used could be misconstrued.
By referring only to `symbols' they risked conflating source symbols and code symbols.
Nonetheless, the job of philosophers, as Bar-Hillel and Carnap saw it, was to provide a theory of \textit{semantic information} that would capture the aspect Shannon ignored.
They explicitly distinguish two kinds of theory, implying a distinction between two entities or concepts.
% Both concepts are indicated by the term `information', but the theory of information pursued by mathematicians and engineers targets a different kind of entity from that which philosophers seek.

Bar-Hillel and Carnap's exposition had significant influence.
\citet[p. 241, n.
1]{dretske1981knowledge} cited them as the best-known sceptics about the relevance of communication theory for philosophical questions about content.
Dretske also offered an interpretation of {\sc Shannon's Warning}:

\begin{myquote}
Communication theory does not tell us what information is.
It ignores questions having to do with the \emph{content} of signals, what \emph{specific information} they carry, in order to describe \emph{how much} information they carry.
In this sense Shannon is surely right: the semantic aspects are irrelevant to the engineering problems.
\par\hspace*{\fill}\citet[41]{dretske1981knowledge}, emphasis original
\end{myquote}

\noindent Two things are worth noticing.
First, Dretske is talking about the content of \textit{signals}, whereas {\sc Shannon's Warning} concerns the semantic properties of source strings.
Second, Dretske repeats Weaver's misquote of Shannon: ``\textit{the} semantic aspects'' instead of ``\textit{these} semantic aspects'' (strictly speaking, Dretske does not use quotation marks -- but earlier on the same page he repeats the misquote along with an endnote reference to Shannon's original statement).
Influenced by Dretske, \citet{dennett1983intentional} repeated Bar-Hillel and Carnap's call for a distinction between mathematical and semantic information:

\begin{myquote}
A more or less standard way of introducing the still imperfectly understood distinction between these two concepts of information is to say that Shannon-Weaver theory measures the \emph{capacity} of information-transmission and information-storage vehicles, but is mute about the \emph{contents} of those channels and vehicles, which will be the topic of the still-to-be-formulated theory of semantic information.
\par\hspace*{\fill}\citet[344]{dennett1983intentional}, emphasis original
\end{myquote}

\noindent Dennett speaks of ``channels and vehicles'', which would presumably include signals.
Like Dretske, the version of the warning operative here is different from the original statement.
\citet[$\S$6]{dennett2017bacteria} is still pursuing this line, and it is nowadays standard to distinguish between two senses of the term `information' in scientific applications.
The Stanford encyclopedia entry `Biological Information' is organised around the distinction, using the labels ``Shannon's concept of information'' and ``Teleosemantic and other richer concepts'' \citep{godfrey-smith2016biological}.
\citet[21]{piccinini2011information} say ``Shannon information does not capture, nor is it intended to capture, the semantic content, or meaning, of signals,'' again focusing on signals rather than source strings.
It is accepted practice to refer to Shannon's formal tools as unrelated to semantic content without further argument: ``I will interpret ‘information’ as ‘semantic information’ (i.e. semantic content), not as Shannon information'' \citep[p. 12 n. 14]{artiga2020signals}; ``Shannon offers no analysis of the relation in virtue of which a sign carries information \textit{about} a state of affairs (his interest was in other issues)''  \citep[p. 7, emphasis original]{neander2017mark}; ``One of the most cited quotes by Shannon is that referred to the independence of his theory with respect to semantic issues [...] Shannon’s theory, taken in itself, is purely quantitative: it ignores any issue related to informational content'' \citep[1988-9]{lombardi2015shannon}; see also \citet[6]{cao2020new} and \citet[1]{kolchinsky2018semantic}.

It could plausibly be argued that the distinction these scholars are aiming for is between semantic content and mutual information.
Mutual information is a measure of correlation, related to surprisal but formally different from it.
The authors cited above could be read as claiming that signals can bear mutual information without possessing semantic content.
I discuss such claims in the second half of the paper; my point here is just that insofar as these writers appeal to Shannon and Weaver to justify their position, they have misinterpreted the communication theorists' remarks.
Weaver's quote above uses `information' as synonymous with surprisal, not mutual information; both he and Shannon were referring to a property of source strings, not a correlation between signals and signifieds.
To my knowledge, no explicit argument has been offered that moves from the original version of {\sc Shannon's Warning} to the conclusion that there exists a property of signals, invoked in communication theory, that is distinct from the concept sought by a philosophical theory of content.

The collective misconstrual would not matter if not for the fact that the mutated form of {\sc Shannon's Warning} is false by the lights of communication theorists themselves.
It is also false by the lights of teleosemantics, a popular theory of content.
According to both teleosemantics and contemporary communication theory, signals in the central model have semantic content.
What is more, their content is directly relevant to the engineering problem of communication, as we shall now see.


%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%
\subsection{The content of a signal in the central model is a source string}\label{subsec:signalContent}

Contemporary discussion of the relevance of communication theory for philosophical approaches to content focuses on signals.
But nothing about signals can be concluded directly from {\sc Shannon's Warning}, which concerns source strings only.
The mathematical tools of communication theory are indeed blind to the meaning of source strings, but interpretations of the theory require that signals are meaningful: the content of a signal is the identity of a source string.
(For simplicity I assume a one-to-one mapping between source strings and code strings; my substantive points are not affected by loosening this constraint.)
Philosophers have conflated the true claim that source strings need not have semantic content with the false claim that well-functioning signals need not have semantic content.

Why do I say central model signals have content?
Because communication theorists say that they do.
Describing Morse code, \citet[385]{shannon1948mathematicalc} says that letters are ``represented by'' sequences of dots and dashes.
In their widely cited textbook \citet[105]{cover2006elements} say the same.
Shannon further uses the locution ``represented by'' in this context on pages 402 and 405; Cover \& Thomas use this and related terms (including calling encodings ``representations'') on pages 5-6, 130, 134, 218-9, 221 and 301.
Obviously the authors cannot be interpreted as taking a stance on philosophical theories of content.
Nevertheless, their usage is evidence of a pretheoretic notion of representation at work -- perhaps something as weak as denotation or indication, but clearly a relation of signification linking signals with source strings.

Furthermore, standard interpretations of the theorems of communication theory require that signals represent source strings.
Consider the \textbf{first theorem} of communication theory, often called the source coding theorem \citep[$\S$5]{cover2006elements} \citep[$\S$4]{mackay2003information}.
The theorem gives a lower bound on the average number of binary symbols required to encode source strings of a certain length.
% This in turn can be used to define the minimum required size of a signal sent through a noiseless channel to perfectly reproduce the sequence of outcomes.
The fewest binary symbols required is equal to a probabilistic measure of the source called its \textbf{entropy}.
The theorem clearly embodies a notion of signification or reference: both the question that prompted the theorem (how many symbols are required?) and the result it offers (the entropy of the source) assume that the symbols are being used to record the outputs of the source.
Cover \& Thomas make this clear:

\begin{myquote}
This theorem provides another justification for the definition of entropy rate -- it is the expected number of bits [code symbols] per [source] symbol required to describe the [source] process.
\par\hspace*{\fill}\citet[115]{cover2006elements}
\end{myquote}

\begin{myquote}
We can design source codes for the most efficient representation of the data. [...] The common representation for all kinds of data uses a binary alphabet. Most modern communication systems are digital, and data are reduced to a binary representation for transmission over the common channel.
\par\hspace*{\fill}\citet[218]{cover2006elements}
\end{myquote}

\noindent The extent to which Cover \& Thomas's use of terms like ``describe'' and ``represent'' corresponds with philosophers' notions of semantic content has not to my knowledge been asked.
Their usage is evidence that some notion of signification is required to make sense of the theorem.
A similar sentiment is found in MacKay's textbook, wherein he models a scientist's experimental setup as a source and the results of the experiment as source outcomes:

\begin{myquote}
We now discuss the information content [entropy] of a source by considering how many bits [code symbols] are needed to describe the outcome of an experiment.
\par\hspace*{\fill}\citet[73]{mackay2003information}
\end{myquote}

\noindent Again, the sense of the entropy measure is intimately bound up with representation, in this case in describing the outcome of the experiment.
Similarly, in a discussion of a special case of the theorem, \citet[397]{shannon1948mathematicalc} speaks of ``the number of bits [code symbols] required to specify the sequence'' of outcomes of the source process.
% It should be clear that ``specify[ing] the sequence'' means bearing the kind of relation teleosemantics requires a signal to bear to its signified.
% What is more, communication theory provides tools for quantifying the severity of misrepresentation \citep{martinez2020information}.
% \textit{Rate-distortion theory} describes trade-offs between sender effort and receiver accuracy \citep{shannon1959coding}.
% Such results would be uninterpretable without appealing to misrepresentation, or falsity, in signals.
% Sceptics might wish to argue that this is not the \emph{same} sense of falsity required for a theory of content, but they would have to deny teleosemantics.
% Since rate-distortion theory has been all but ignored in the relevant philosophical literature, they would have to present novel arguments in order to do so.

Communication theorists from Shannon to contemporary textbook authors make use of a notion of semantic content, as indicated by their use of words like `represent', `describe' and `specify'.
There is an intuitive sense to this usage.
It must be the case that signals in the central model bear some exploitable relation to source strings.
If the decoder is to perform properly, it must be able to reconstruct the original string from the signal.
There must therefore be some sense in which the signal indicates or refers to the source string.
The specific mapping from signal to source -- the `semantics' of the signalling system -- is determined by the encoding scheme.

Teleosemantics puts theoretical legs under this intuition.
The central model (figure \ref{fig:central}) is a special case of the basic teleosemantic model (figure \ref{fig:teleo}; here and throughout I refer to Millikan's \parencite*[$\S$6]{millikan2004varieties} teleosemantic theory).
Teleosemantics attributes semantic content to signals when they are intermediaries between cooperating senders and receivers, such that receivers condition their behaviour on the form of the intermediary.
Senders and receivers cooperate when they share a proper function, an outcome they have been designed to bring about.
The content of a signal is the state of affairs that, together with the receiver's response to the signal, produces outcomes that satisfy the sender-receiver partnership's shared proper function.
In figure \ref{fig:teleo} a distal state sits causally upstream of a target that the receiver has causal influence over.
The signal has this distal state as its content, because the receiver can achieve greater success by conditioning its act on the signal.

\input{fig_teleo}

In the central model, source strings play the role of both distal and proximate states (though more recent models in communication theory distinguish them, e.g. \citet{berger1996ceo}).
Signalling games typically collapse those states too (figure \ref{fig:sr}).
Applying the basic teleosemantic model to the central model, the encoder is a sender and the decoder is a receiver.
Sender and receiver share a proper function as a consequence of design: to reconstruct the source string.\footnote{To clarify: the sender's most immediate proper function is to produce a signal that bears a certain relation to the source sequence. But it has this function in part because of its slightly less immediate function, shared with the receiver, of reconstructing the source string at the target.}
The basic teleosemantic model does not include noise, but adding a noise variable would not affect the definition of semantic content.
The basic teleosemantic model makes explicit the fact that acts are judged successful or unsuccessful depending on a distal state.
This `Success?' variable, and the causal link to it from the relevant distal state, is omitted from figures \ref{fig:central} and \ref{fig:sr}; nonetheless, in both the central model and cooperative signalling games the receiver's act together with a distal state determines the joint success of the signalling partnership (via an error measure or payoff matrix).

\input{fig_sr}

Every component of the basic teleosemantic model is present in the central model of communication theory.
Teleosemantics therefore says that the content of a signal is the source string it encodes.
That is not because the source string is causally upstream of the encoder, but because the source string is the distal state that determines whether or not the decoder-as-receiver is successful.
% Different source strings require different decoder acts to achieve success.
Consider a system that transmits outcomes of coin tosses $\{H,T\}$.
Each time the coin is tossed at the source, the eventual task of the decoder is to produce the appropriate symbol $H$ or $T$ matching the result of the toss.
Suppose the sender transmits signals according to the code $H\rightarrow1,\ T\rightarrow0$, and the result of three coin tosses is $H, T, H$.
Then, assuming no noise, when the decoder receives the signal $101$ it correctly produces the sequence $HTH$.
Communication is successful because the reproduced string is identical to the original sequence of outcomes.
The signal $101$ represents the source sequence $HTH$, and that is how the decoder successfully reproduces it.\footnote{The claim is not that the content of the signal explains how the receiver produces the string `HTH' at the target. The claim is that the content of the signal explains how the receiver successfully reproduces the source string at the target. In general, teleosemantics claims not that content explains behaviour, but that content explains success. Figure \ref{fig:teleo} makes that explicit by distinguishing the Act variable from the Success? variable.}

It might be objected that this is a very thin notion of `representation'.
Surely philosophers who aim to give a naturalist account of representation are looking for something much richer than the relationship between simple electronic signals and their signifieds?
Indeed, the liberality debate constitutes an ongoing discussion about this very question \citep{artiga2016liberal,desouzafilho2022dual}.
Teleosemantics is particularly susceptible to the charge of being too liberal in its attribution of representational status, because the conditions implied by figure \ref{fig:teleo} are very easy to satisfy.
My argument here does not serve to defend teleosemantics against this charge.
I am only trying to demonstrate that teleosemantics attributes content to central model signals.
Detractors may treat this as a mark against teleosemantics; so be it.
My point is just that the sceptic cannot appeal to {\sc Shannon's Warning} to make their case, because Shannon was talking about source strings rather than signals.

% Furthermore, 
% If a 1 is transmitted when the coin comes up tails, the signal is false.
% The decoder will fail in its objective to match the symbol it produces with the outcome of the coin toss: it will produce an $H$ when it ought to have produced a $T$.
% This failure is attributable to the signal being false.
% In general, a signal is correct to the extent that it accords with the encoding procedure with which the system was designed.

To sum up, by dint of joint design, encoder and decoder have a shared proper function to reconstruct the source string at the target.
They do this by means of an intermediary -- the code string as signal.
Teleosemantics identifies the relation between signal and source string as the basic form of semantic content.
This cashes out the pretheoretic usage of terms like `represent', `describe' and `specify' used by communication theorists.

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Is it really a semantic relation?}

% The relation that enables proper functioning of the decoder is sometimes taken to be purely syntactic.
% But this view begs the question against teleosemantics.
% It would be possible to run the argument given here as a modus tollens against teleosemantics, and thus make such a case.
% But those scholars who deny the relevance of communication theory for the naturalist project do not typically push this line.
% An exception is \citet[$\S$4]{hutto2013radicalizing}, who demand more than merely proper functioning from a definition of semantic content.
% They assert that semantic content has \textit{intensionality}, a feature beyond the simple relation between signal and signified.
% If semantic content requires intensionality, and the signal-source string relation in the central model does not possess intensionality, then teleosemantics is in trouble.
% A similar idea is proposed by \citet{lean2014shannon}, who does not give specific conditions on semantic content but asserts that it is distinct from the notion of Shannon information, which he takes to be a relation borne between signals and world affairs that explains signals' proper functioning.

% I have argued elsewhere that semantic content does not require intensionality, and that if distinctions like Lean's are to tell against teleosemantics, they must provide clear conditions on semantic content that teleosemantics fails to meet [CITATION REDACTED FOR ANONYMITY].
% These objections are, I think, based on the broadly correct idea that paradigmatic examples of semantic content are far richer than signals in the central model.
% Human linguistic utterances possess many more philosophically relevant features than do strings of 1s and 0s.
% But these objections move from this observation to the conclusion that the teleosemantic approach to the naturalist project cannot be the correct one.
% I claim this move is unjustified.
% Of more relevance for the present paper is a similar move, to the conclusion that communication theory cannot aid the naturalist project.
% This move is similarly unjustified -- and is outright denied by arguments that demonstrate the relevance of the theory for concepts of representation in cognitive science \citep{martinez2019representations}.

% Let me conclude this section by zooming out slightly.
% Bar-Hillel and Carnap, and many since them, have argued or assumed that the theory of semantic content sought by philosophers must be distinct from communication theory.
% In the background of my rebuttal to their claims is the idea that the eventual semantic theory will instead be a \textit{generalisation} of communication theory.
% Or to put it the other way round, communication theory will turn out to be a special case of a broader theory of representation.
% If teleosemantics is to be that theory, it should eventually be translated into a formal language comparable with that of Shannon's theory.
% Obviously I cannot pursue that project here, but its conception should at least give the reader a sense of the positive position underlying my negative assertions.

\subsection{A sceptical riposte: symbol manipulation does not bestow content}

Encoding, the sceptic will notice, is the transformation of symbols from one lexicon into another.
Since I am claiming it is the encoding scheme that confers content, my position appears to entail that any process by which symbols of one lexicon are converted into symbols of another confers content.
That is a big problem: it is implausible that manipulating symbols from lexicon $L_1$ into lexicon $L_2$ bestows the symbols of $L_2$ with the content `symbol such-and-such from $L_1$'.
Symbol manipulation is a matter of syntax, not semantics.
If that is all that is happening in an encoding scheme, then it is implausible that central model signals really do have the contents I ascribe.

To respond, I accept the following premise:

\begin{myquote}
\smi{}: Converting symbols of lexicon $L_1$ into symbols of lexicon $L_2$ does not bestow the symbols of $L_2$ with content.
\end{myquote}

\noindent I think we can all agree on \smi{}.
Teleosemantics is liberal, but not that liberal.
The question is whether symbol manipulation is all there is to encoding.
While the term `encoding' might be used in different ways in different branches of science and philosophy, including in ways that imply only symbol manipulation, I contend that its use in communication theory implies something stronger.
Encoded strings are produced as part of a sender-receiver system, in order to be decoded during performance of a joint function.
That makes a difference, because it ensures the system fits the teleosemantic template.
Shuffling symbols does not bestow content, but joint design of sender and receiver does.

Furthermore, although I have focused on symbols in the exposition so far, it turns out that source outcomes need not be symbols at all.
They could be dance steps, military manoeuvres, restaurant locations; anything over which a probability distribution can be defined.
It also turns out that the actions of the receiver need not be exact duplicates of the outcomes at the source; they need only be actions that, combined with source outcomes, yield a cost function for the system as a whole.
If this sounds like the sender-receiver framework associated with \citet{skyrms2010signals} and \citet{lewis1969convention}, that's because it is formally equivalent to it \citep{martinez2019deception}; see again figures \ref{fig:central} and \ref{fig:sr}.
Source and target need not be symbols.
They are just commonly described as such because that is part of the typical use case of communication theory.
The mathematics does not demand that signals be about symbols from a lexicon: they can be about anything at all.

To summarise the entire section, arguments denying the relevance of communication theory for philosophy based on {\sc Shannon's Warning} do not hold water.
A stronger argument adverts to the breadth of application of another of Shannon's mathematical measures: mutual information.
It is to this point I now turn.

%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%

% \subsection{Rejoinder: information in signals does not distinguish world states}

% An argument along these lines seems to be what Scarantino has in mind in the following passage:

% \begin{myquote}
% [W]e cannot fully capture the informational content of a signal by merely listing the amounts of incremental information being carried. Informative signals do not tell us just \emph{how much} probabilities have changed; they also tell us what are the \emph{states of affairs} that had their probabilities changed. On Skyrms's account, two signals that carry the \emph{same amount} of incremental information with respect to four \emph{completely different} states of affairs would have the \emph{same informational content}, captured by, say, the vector $<1,\ 3.4,\ -\infty,\ 1>$. This is unacceptable.
% \par\hspace*{\fill}\citet[p.429, emphasis original]{scarantino2015information}
% \end{myquote}

% Scarantino objects that statistical content vectors omit the referents of signals because they contain only numbers.
% Two signals bearing quantitatively similar probabilistic relations to different states of affairs could have the same content.
% In much the same way, two objects bearing quantitatively similar spatial and mass-ratio relations to different gravitational bodies could have the same weight.
% Pursuing the analogy, we might imagine 17th century metaphysicians interested in the relation between everyday objects and the earth, or between the earth and the sun.
% Suppose they learned of Newton's work, but objected that although he managed to quantify gravitational force, something about the relation of interest was omitted from his calculations, because two distinct objects might have the same weight.
% Such a view would be at best unfair, at worst absurd.

% Scarantino believes statistical content vectors ought to contain an explicit term that differentiates one event space from another.
% But that is not how the relevant mathematical terminology works.
% Each content vector has an already defined event space $W$, just as each weight measurement is made with respect to an already specified planetary body.
% Specification of a sign's statistical content vector ineliminably includes its referent.
% I suspect that if Scarantino's concerns were valid, it would \emph{never} be possible for a specification of content to pick out the state of affairs that is the sign's referent.
% After all, we must construct a definition of content using only the terms available in our language.
% Following Scarantino, we might include a phrase such as `eagle present' in our content specification.
% But such terms do not \emph{by themselves} indicate the state of affairs; we require, by Scarantino's logic, an extra piece of terminology that gives the correct interpretation of the term.
% But we would then require an interpretation of the interpretation, and so on.\footnote{This regress seems related to the Kripke-Wittgenstein paradox \citep[$\S$198ff]{wittgenstein1953philosophical} \citep{kripke1982wittgenstein}.}
% Scarantino's argument is predicated on a confusion about what must be explicitly versus implicitly stated in a formal definition.


% It is not possible to move from the claim that `different signals can have the same number of bits' to the claim that `there are two different concepts of information'.
% It is not possible for the same reason that it is not possible to move from the claim that `different objects can have the same weight' to the claim that `there are two different concepts of weight'.
% My point is not (just) that communication theory provides tools for measuring the relations that teleosemantics claims ground semantic content.
% My point is that my opponent cannot possibly have established the counter-claim.
% If they could, they would be able to establish similar claims for volume and weight.
% % The analogies show that such results would be absurd.